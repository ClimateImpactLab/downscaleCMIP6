{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import collections\n",
    "import os \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import xesmf as xe\n",
    "\n",
    "from utils import _convert_lons, _remove_leap_days, compute_daily_climo, calculate_anomaly\n",
    "from regridding import apply_weights\n",
    "\n",
    "import dask.distributed as dd\n",
    "import dask_kubernetes as dk\n",
    "import dask\n",
    "import rhg_compute_tools.kubernetes as rhgk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is a test of all the steps for Spatial Disaggregation to get a handle on the total CPU time it will take for this part of BCSD. \n",
    "\n",
    "Once-off steps: \n",
    "\n",
    "1. compute multi-decade daily climatologies of ERA-5 at obs-res and coarsen it to model-res (they, e.g. NASA-NEX, do not say how, we will do bilinear for consistency with later step)\n",
    "\n",
    "Per model/scenario/experiment steps:\n",
    "\n",
    "1. subtract (or divide for precip) BC’ed model data at model-res from obs climo at model resolution to calculate a “scaling factor” \n",
    "2. bilinearly interpolate “scaling factor” (using xESMF) from the model grid to the obs grid \n",
    "3. Apply scaling factor by adding (for temp) and multiplying (for precip) the “scaling factor” to the obs-res daily climatology \n",
    "\n",
    "NOTE: For the purpose of being conservative with timing, the \"coarsen obs climatology step to model-res\" is in the per model/scenario/experiment step, since we don't know for sure how/if CMIP6 models will be at exactly the same resolution. \n",
    "\n",
    "Currently this workflow is only built out for temperature, not precipitation. All steps are included, the last step (applying the interpolated scale factor to the obs-res daily climatology) has not yet been tested. All other parts of the workflow have been tested. The second to last step, the interpolation of the scaling factor from coarse to fine, is the most memory intensive, thus I have only tested for a subset of timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853725797cc74cc293754dd859343000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>KubeCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client, cluster = rhgk.get_standard_cluster()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load test bias corrected output from global bias correction prototype notebook (BC'ed NASA GISS CMIP6 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_ds_longitude(ds, lon_name='longitude'):\n",
    "    ds_new = ds.assign_coords(lon=(((ds[lon_name] + 180) % 360) - 180)).sortby(lon_name)\n",
    "    return ds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = '/gcs/rhg-data/climate/downscaled/workdir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_model = xr.open_dataset(os.path.join(workdir, \n",
    "                                          'global_bias_correction_scaling_test.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_model = tmax_model.loc[dict(time=slice(\"%s-01-01\" %str(year), \"%s-12-31\" %str(year)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_obs = xr.open_dataset(os.path.join('/gcs/rhg-data/climate/source_data/GMFD/tmax', \n",
    "                                         'tmax_0p25_daily_1990-1990.nc')).squeeze(drop=True\n",
    "                                          ).rename({'latitude': 'lat', 'longitude': 'lon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize longitudes \n",
    "tmax_obs = _convert_ds_longitude(tmax_obs, lon_name='lon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove leap days from obs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leap days \n",
    "tmax_obs = _remove_leap_days(tmax_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load daily obs climatology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "climo_obs_fine = (xr.open_dataset(os.path.join(workdir, 'gmfd_test_climo.nc'))\n",
    "                 .rename({'latitude': 'lat', 'longitude': 'lon'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate obs climo: fine -> coarse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuse existing file: /gcs/rhg-data/climate/downscaled/workdir/obs_to_mod_bilinear_spatial_disagg.nc\n",
      "CPU times: user 30.5 ms, sys: 19.3 ms, total: 49.8 ms\n",
      "Wall time: 197 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "obs_to_mod_weights = workdir + '/obs_to_mod_bilinear_spatial_disagg.nc'\n",
    "regridder_obs_to_mod = xe.Regridder(tmax_obs.isel(time=0), tmax_model.isel(time=0), \n",
    "                         'bilinear', filename=obs_to_mod_weights, reuse_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 3.05 s, total: 4.15 s\n",
      "Wall time: 9.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "climo_obs_coarse_lazy = xr.map_blocks(apply_weights, regridder_obs_to_mod, \n",
    "                                args=[climo_obs_fine['tmax']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 417 µs, sys: 0 ns, total: 417 µs\n",
      "Wall time: 423 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "climo_obs_coarse = climo_obs_coarse_lazy.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute scaling factor by subtracting for temperature, dividing for precip, the BC'ed model data at model-res from obs climo at model-res. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {'lat': 75, 'lon': 75}\n",
    "climo_obs_coarse = climo_obs_coarse.chunk(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.4 s, sys: 898 ms, total: 19.3 s\n",
      "Wall time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "scale_factor_coarse = xr.map_blocks(calculate_anomaly, \n",
    "                                    tmax_model, args=[climo_obs_coarse, 'tasmax'])\n",
    "sfc = scale_factor_coarse.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate scaling factor: coarse (model grid) -> fine (obs grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuse existing file: /gcs/rhg-data/climate/downscaled/workdir/mod_to_obs_bilinear_spatial_disagg.nc\n",
      "CPU times: user 103 ms, sys: 196 ms, total: 299 ms\n",
      "Wall time: 950 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mod_to_obs_weights = workdir + '/mod_to_obs_bilinear_spatial_disagg.nc'\n",
    "regridder_mod_to_obs = xe.Regridder(tmax_model.isel(time=0), \n",
    "                                    tmax_obs.isel(time=0), \n",
    "                         'bilinear', filename=mod_to_obs_weights, reuse_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.45 s, sys: 4.17 s, total: 8.62 s\n",
      "Wall time: 8.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sfc = sfc.drop('dayofyear')\n",
    "sff_lazy = xr.map_blocks(apply_weights, regridder_mod_to_obs, \n",
    "                                args=[sfc])\n",
    "sff_lazy_compute = sff_lazy.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add (or multiply for precip) the scaling factor to the obs-res daily climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sff_ds = sff_lazy_compute.to_dataset(name='scale_factor_fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scale_factor(da, obs_climo, groupby_type):\n",
    "    \n",
    "    '''if sum(ds.shape) == 0:\n",
    "        return ds'''\n",
    "    \n",
    "    sff_daily = da.groupby(groupby_type)\n",
    "    return sff_daily + obs_climo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''year = 1990\n",
    "sff_ds_year = sff_ds.sel(time=sff_ds.time.dt.year.isin([year]))\n",
    "mod_yr_downscaled = apply_scale_factor(sff_ds_year['scale_factor_fine'], climo_obs_fine['tmax'], \n",
    "                                       sff_ds_year.time.dt.dayofyear)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''filepath = '/home/jovyan/spatial_disagg/sd_%s.nc' %str(year)\n",
    "mod_yr_ds = mod_yr_downscaled.to_dataset(name='downscaled')\n",
    "mod_yr_ds.to_netcdf(filepath)\n",
    "print(\"finished %s\" %str(year))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in np.unique(sff_ds.time.dt.year):\n",
    "    sff_ds_year = sff_ds.sel(time=sff_ds.time.dt.year.isin([year]))\n",
    "    mod_yr_downscaled = apply_scale_factor(sff_ds_year['scale_factor_fine'], climo_obs_fine['tmax'], \n",
    "                                           sff_ds_year.time.dt.dayofyear)\n",
    "    filepath = workdir + '/spatial_disagg_prototype/%s.nc' %str(year)\n",
    "    mod_yr_downscaled.to_netcdf(filepath)\n",
    "    print(\"finished %s\" %str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.4 s, sys: 14.6 ms, total: 4.42 s\n",
      "Wall time: 4.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sff_chunks = {'time': 15}\n",
    "sff_ds = sff_ds.chunk(sff_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.2 s, sys: 1.67 s, total: 58.9 s\n",
      "Wall time: 55.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "model_ds = xr.map_blocks(apply_scale_factor, sff_ds['scale_factor_fine'], \n",
    "                         args=[climo_obs_fine['tmax'], sff_ds.time.dt.dayofyear], \n",
    "                         template=sff_ds['scale_factor_fine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_downscaled = model_ds.to_dataset(name='downscaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_downscaled.to_netcdf(workdir + '/spatial_disagg_prototype/%s.nc' %str(year))\n",
    "print(\"finished writing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply standardizing functions for final output and save (probably as zarr array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
