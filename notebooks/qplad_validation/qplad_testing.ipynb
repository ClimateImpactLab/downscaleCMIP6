{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://github.com/ClimateImpactLab/dodola.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from dodola.core import (\n",
    "    train_quantiledeltamapping,\n",
    "    adjust_quantiledeltamapping,\n",
    "    train_analogdownscaling,\n",
    "    adjust_analogdownscaling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qplad_integration_af_quantiles():\n",
    "    \"\"\"\n",
    "    Test QPLAD correctly matches adjustmentfactor and quantiles for lat and dayofyear\n",
    "    The strategy is to bias-correct a Dataset of ones, and then try to\n",
    "    downscale it to two gridpoints with QPLAD. In one case we ta take the\n",
    "    adjustment factors for a single dayofyear and manually change it to\n",
    "    0.0. Then check for the corresponding change in the output dataset. In\n",
    "    the other case we take the adjustment factors for one of the two\n",
    "    latitudes we're downscaling to and manually change it to 0.0. We then\n",
    "    check for the corresponding change in the output dataset for that latitude.\n",
    "    \"\"\"\n",
    "    kind = \"*\"\n",
    "    lat = [1.0, 1.5]\n",
    "    time = xr.cftime_range(start=\"1994-12-17\", end=\"2015-01-15\", calendar=\"noleap\")\n",
    "    variable = \"scen\"\n",
    "\n",
    "    data_ref = xr.DataArray(\n",
    "        np.ones((len(time), len(lat)), dtype=\"float64\"),\n",
    "        ## If you want something more sophisticated:\n",
    "        # np.arange(len(time) * len(lat), dtype=\"float64\").reshape(len(time), len(lat)),\n",
    "        coords={\"time\": time, \"lat\": lat},\n",
    "        attrs={\"units\": \"K\"},\n",
    "        name=variable,\n",
    "    ).chunk({\"time\": -1, \"lat\": -1})\n",
    "    data_train = data_ref + 2\n",
    "    data_train.attrs[\"units\"] = \"K\"\n",
    "\n",
    "    ref_fine = data_ref.to_dataset()\n",
    "    ds_train = data_train.to_dataset()\n",
    "\n",
    "    # take the mean across space to represent coarse reference data for AFs\n",
    "    ds_ref_coarse = ref_fine.mean([\"lat\"], keep_attrs=True)\n",
    "    ds_train = ds_train.mean([\"lat\"], keep_attrs=True)\n",
    "\n",
    "    # tile the fine resolution grid with the coarse resolution ref data\n",
    "    ref_coarse = ds_ref_coarse.broadcast_like(ref_fine)\n",
    "    ds_bc = ds_train\n",
    "    ds_bc[variable].attrs[\"units\"] = \"K\"\n",
    "\n",
    "    # this is an integration test between QDM and QPLAD, so use QDM services\n",
    "    # for bias correction\n",
    "    target_year = 2005\n",
    "    qdm_model = train_quantiledeltamapping(\n",
    "        reference=ds_ref_coarse, historical=ds_train, variable=variable, kind=kind\n",
    "    )\n",
    "    biascorrected_coarse = adjust_quantiledeltamapping(\n",
    "        simulation=ds_bc,\n",
    "        variable=variable,\n",
    "        qdm=qdm_model.ds,\n",
    "        years=[target_year],\n",
    "        include_quantiles=True,\n",
    "    )\n",
    "\n",
    "    # make bias corrected data on the fine resolution grid\n",
    "    biascorrected_fine = biascorrected_coarse.broadcast_like(\n",
    "        ref_fine.sel(\n",
    "            time=slice(\"{}-01-01\".format(target_year), \"{}-12-31\".format(target_year))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    qplad_model = train_analogdownscaling(\n",
    "        coarse_reference=ref_coarse,\n",
    "        fine_reference=ref_fine,\n",
    "        variable=variable,\n",
    "        kind=kind,\n",
    "    )\n",
    "\n",
    "    # TODO: These prob should be two separate tests with setup fixtures...\n",
    "    spoiled_time = qplad_model.ds.copy(deep=True)\n",
    "    spoiled_latitude = qplad_model.ds.copy(deep=True)\n",
    "\n",
    "    # Spoil one dayoftheyear value in adjustment factors (force it to be 0.0)\n",
    "    # and test that the spoiled value correctly propigates through to output.\n",
    "    time_idx_to_spoil = 25\n",
    "    spoiled_time[\"af\"][:, time_idx_to_spoil, :] = 0.0\n",
    "    qplad_model.ds = spoiled_time\n",
    "    downscaled = adjust_analogdownscaling(\n",
    "        simulation=biascorrected_fine.set_coords(\n",
    "            [\"sim_q\"]\n",
    "        ),  # func assumes sim_q is coordinate...\n",
    "        qplad=qplad_model,\n",
    "        variable=variable,\n",
    "    )\n",
    "\n",
    "    # All but two values should be 1.0...\n",
    "    assert (downscaled[variable].values == 1.0).sum() == 728\n",
    "    # We should have 2 `0.0` entires. One in each lat...\n",
    "    assert (downscaled[variable].values == 0.0).sum() == 2\n",
    "    # All our 0.0s should be in this dayofyear/time slice in output dataset.\n",
    "    np.testing.assert_array_equal(\n",
    "        downscaled[variable].values[time_idx_to_spoil, :], np.array([0.0, 0.0])\n",
    "    )\n",
    "\n",
    "    # Similar to above, spoil one lat value in adjustment factors\n",
    "    # (force it to be 0.0) and test that the spoiled value correctly\n",
    "    # propigates through to output.\n",
    "    spoiled_latitude = qplad_model.ds.copy(deep=True)\n",
    "    laitutde_idx_to_spoil = 0\n",
    "    spoiled_latitude[\"af\"][laitutde_idx_to_spoil, ...] = 0.0\n",
    "    qplad_model.ds = spoiled_latitude\n",
    "    downscaled = adjust_analogdownscaling(\n",
    "        simulation=biascorrected_fine.set_coords(\n",
    "            [\"sim_q\"]\n",
    "        ),  # func assumes sim_q is coordinate...\n",
    "        qplad=qplad_model,\n",
    "        variable=variable,\n",
    "    )\n",
    "    # Half of values in output should be 1.0...\n",
    "    assert (downscaled[variable].values == 1.0).sum() == 364\n",
    "    # The other half should be `0.0` due to the spoiled data...\n",
    "    assert (downscaled[variable].values == 0.0).sum() == 366\n",
    "    # All our 0.0s should be in this single lat in output dataset.\n",
    "    assert all(downscaled[variable].values[:, laitutde_idx_to_spoil] == 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# All but two values should be 1.0...\\nassert (downscaled[variable].values == 1.0).sum() == 728\\n# We should have 2 `0.0` entires. One in each lat...\\nassert (downscaled[variable].values == 0.0).sum() == 2\\n# All our 0.0s should be in this dayofyear/time slice in output dataset.\\nnp.testing.assert_array_equal(\\n    downscaled[variable].values[time_idx_to_spoil, :], np.array([0.0, 0.0])\\n)'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test QPLAD correctly matches adjustmentfactor and quantiles for lat and dayofyear\n",
    "The strategy is to bias-correct a Dataset of ones, and then try to\n",
    "downscale it to two gridpoints with QPLAD. In one case we ta take the\n",
    "adjustment factors for a single dayofyear and manually change it to\n",
    "0.0. Then check for the corresponding change in the output dataset. In\n",
    "the other case we take the adjustment factors for one of the two\n",
    "latitudes we're downscaling to and manually change it to 0.0. We then\n",
    "check for the corresponding change in the output dataset for that latitude.\n",
    "\"\"\"\n",
    "kind = \"*\"\n",
    "lat = [1.0, 1.5]\n",
    "time = xr.cftime_range(start=\"1994-12-17\", end=\"2015-01-15\", calendar=\"noleap\")\n",
    "variable = \"scen\"\n",
    "\n",
    "data_ref = xr.DataArray(\n",
    "    np.ones((len(time), len(lat)), dtype=\"float64\"),\n",
    "    ## If you want something more sophisticated:\n",
    "    # np.arange(len(time) * len(lat), dtype=\"float64\").reshape(len(time), len(lat)),\n",
    "    coords={\"time\": time, \"lat\": lat},\n",
    "    attrs={\"units\": \"K\"},\n",
    "    dims=[\"time\", \"lat\"],\n",
    "    name=variable,\n",
    ").chunk({\"time\": -1, \"lat\": -1})\n",
    "data_train = data_ref + 2\n",
    "data_train.attrs[\"units\"] = \"K\"\n",
    "\n",
    "ref_fine = data_ref.to_dataset()\n",
    "ds_train = data_train.to_dataset()\n",
    "\n",
    "# take the mean across space to represent coarse reference data for AFs\n",
    "ds_ref_coarse = ref_fine.mean([\"lat\"], keep_attrs=True)\n",
    "ds_train = ds_train.mean([\"lat\"], keep_attrs=True)\n",
    "\n",
    "# tile the fine resolution grid with the coarse resolution ref data\n",
    "ref_coarse = ds_ref_coarse.broadcast_like(ref_fine)\n",
    "ds_bc = ds_train\n",
    "ds_bc[variable].attrs[\"units\"] = \"K\"\n",
    "\n",
    "# this is an integration test between QDM and QPLAD, so use QDM services\n",
    "# for bias correction\n",
    "target_year = 2005\n",
    "qdm_model = train_quantiledeltamapping(\n",
    "    reference=ds_ref_coarse, historical=ds_train, variable=variable, kind=kind\n",
    ")\n",
    "biascorrected_coarse = adjust_quantiledeltamapping(\n",
    "    simulation=ds_bc,\n",
    "    variable=variable,\n",
    "    qdm=qdm_model.ds,\n",
    "    years=[target_year],\n",
    "    include_quantiles=True,\n",
    ")\n",
    "\n",
    "# make bias corrected data on the fine resolution grid\n",
    "biascorrected_fine = biascorrected_coarse.broadcast_like(\n",
    "    ref_fine.sel(\n",
    "        time=slice(\"{}-01-01\".format(target_year), \"{}-12-31\".format(target_year))\n",
    "    )\n",
    ")\n",
    "\n",
    "qplad_model = train_analogdownscaling(\n",
    "    coarse_reference=ref_coarse,\n",
    "    fine_reference=ref_fine,\n",
    "    variable=variable,\n",
    "    kind=kind,\n",
    ")\n",
    "\n",
    "# TODO: These prob should be two separate tests with setup fixtures...\n",
    "spoiled_time = qplad_model.ds.copy(deep=True)\n",
    "spoiled_latitude = qplad_model.ds.copy(deep=True)\n",
    "\n",
    "# spoil quantile for one doy \n",
    "# pick day of the year - 100th day - for one latitude\n",
    "doy = 100\n",
    "lat_pt = 0\n",
    "# get the quantile from bias corrected data for this doy and latitude\n",
    "q_100 = biascorrected_fine.sim_q[lat_pt, doy].values \n",
    "# extract quantiles from afs \n",
    "bc_quantiles = qplad_model.ds.af[0, 100, :].quantiles.values\n",
    "# get index of the af for that doy \n",
    "q_idx = np.argmin(np.abs(q_100 - bc_quantiles))\n",
    "\n",
    "# spoil that doy quantile AF \n",
    "spoiled_time = spoiled_time.load()\n",
    "spoiled_time[\"af\"][0, 100, q_idx] = 101\n",
    "qplad_model.ds[\"af\"] = spoiled_time[\"af\"]\n",
    "\n",
    "# Spoil one dayoftheyear value in adjustment factors (force it to be 0.0)\n",
    "# and test that the spoiled value correctly propagates through to output.\n",
    "# time_idx_to_spoil = 25\n",
    "# spoiled_time[\"af\"][:, time_idx_to_spoil, :] = 0.0\n",
    "# spoiled_time[\"af\"][:, time_idx_to_spoil, :].values = 0.0\n",
    "# qplad_model.ds = spoiled_time\n",
    "\n",
    "downscaled = adjust_analogdownscaling(\n",
    "    simulation=biascorrected_fine.set_coords(\n",
    "        [\"sim_q\"]\n",
    "    ),  # func assumes sim_q is coordinate...\n",
    "    qplad=qplad_model,\n",
    "    variable=variable,\n",
    ")\n",
    "\n",
    "'''# All but two values should be 1.0...\n",
    "assert (downscaled[variable].values == 1.0).sum() == 728\n",
    "# We should have 2 `0.0` entires. One in each lat...\n",
    "assert (downscaled[variable].values == 0.0).sum() == 2\n",
    "# All our 0.0s should be in this dayofyear/time slice in output dataset.\n",
    "np.testing.assert_array_equal(\n",
    "    downscaled[variable].values[time_idx_to_spoil, :], np.array([0.0, 0.0])\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 365)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biascorrected_fine.sim_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spoiled_time.af.values[0, 100, :].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(downscaled['scen'].values[lat_pt, :])\n",
    "# qplad_model.ds.af[0, 100, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(downscaled['scen'].values[lat_pt, :])\n",
    "# qplad_model.ds.af[0, 100, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downscaled['scen'].values[lat_pt, :].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spoiled_time[\"af\"][0, 100, q_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.525\n",
      "bias corrected value is 1.0\n",
      "downscaled value is 1.0\n"
     ]
    }
   ],
   "source": [
    "# pick day of the year - 100th day - for one latitude\n",
    "doy = 100\n",
    "lat_pt = 0\n",
    "q_100 = biascorrected_fine.sim_q[lat_pt, doy].values \n",
    "print(q_100)\n",
    "print(\"bias corrected value is {}\".format(biascorrected_fine['scen'].values[lat_pt, doy]))\n",
    "print(\"downscaled value is {}\".format(downscaled['scen'].values[lat_pt, doy]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_quantiles = qplad_model.ds.af[0, 100, :].quantiles.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(np.abs(q_100 - bc_quantiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "this variable's data is stored in a dask array, which does not support item assignment. To assign to this variable, you must first load it into memory explicitly using the .load() method or accessing its .values attribute.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-71a0f1c4f93d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# and test that the spoiled value correctly propigates through to output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mtime_idx_to_spoil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mspoiled_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"af\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_idx_to_spoil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0mqplad_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspoiled_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m downscaled = adjust_analogdownscaling(\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    716\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_key_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             }\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mindexable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0mindexable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_tuple\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/xarray/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m   1380\u001b[0m             \u001b[0;34m\"this variable's data is stored in a dask array, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m             \u001b[0;34m\"which does not support item assignment. To \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: this variable's data is stored in a dask array, which does not support item assignment. To assign to this variable, you must first load it into memory explicitly using the .load() method or accessing its .values attribute."
     ]
    }
   ],
   "source": [
    "kind = \"*\"\n",
    "lat = [1.0, 1.5]\n",
    "time = xr.cftime_range(start=\"1994-12-17\", end=\"2015-01-15\", calendar=\"noleap\")\n",
    "variable = \"scen\"\n",
    "\n",
    "data_ref = xr.DataArray(\n",
    "    np.ones((len(time), len(lat)), dtype=\"float64\"),\n",
    "    coords={\"time\": time, \"lat\": lat},\n",
    "    attrs={\"units\": \"K\"},\n",
    "    dims=[\"time\", \"lat\"],\n",
    "    name=variable,\n",
    ").chunk({\"time\": -1, \"lat\": -1})\n",
    "data_train = data_ref + 2\n",
    "data_train.attrs[\"units\"] = \"K\"\n",
    "\n",
    "ref_fine = data_ref.to_dataset()\n",
    "ds_train = data_train.to_dataset()\n",
    "\n",
    "# take the mean across space to represent coarse reference data for AFs\n",
    "ds_ref_coarse = ref_fine.mean([\"lat\"], keep_attrs=True)\n",
    "ds_train = ds_train.mean([\"lat\"], keep_attrs=True)\n",
    "\n",
    "# tile the fine resolution grid with the coarse resolution ref data\n",
    "ref_coarse = ds_ref_coarse.broadcast_like(ref_fine)\n",
    "ds_bc = ds_train\n",
    "ds_bc[variable].attrs[\"units\"] = \"K\"\n",
    "\n",
    "# this is an integration test between QDM and QPLAD, so use QDM services\n",
    "# for bias correction\n",
    "target_year = 2005\n",
    "qdm_model = train_quantiledeltamapping(\n",
    "    reference=ds_ref_coarse, historical=ds_train, variable=variable, kind=kind\n",
    ")\n",
    "biascorrected_coarse = adjust_quantiledeltamapping(\n",
    "    simulation=ds_bc,\n",
    "    variable=variable,\n",
    "    qdm=qdm_model.ds,\n",
    "    years=[target_year],\n",
    "    include_quantiles=True,\n",
    ")\n",
    "\n",
    "# make bias corrected data on the fine resolution grid\n",
    "biascorrected_fine = biascorrected_coarse.broadcast_like(\n",
    "    ref_fine.sel(\n",
    "        time=slice(\"{}-01-01\".format(target_year), \"{}-12-31\".format(target_year))\n",
    "    )\n",
    ")\n",
    "\n",
    "qplad_model = train_analogdownscaling(\n",
    "    coarse_reference=ref_coarse,\n",
    "    fine_reference=ref_fine,\n",
    "    variable=variable,\n",
    "    kind=kind,\n",
    ")\n",
    "\n",
    "# TODO: These prob should be two separate tests with setup fixtures...\n",
    "spoiled_time = qplad_model.ds.copy(deep=True)\n",
    "spoiled_latitude = qplad_model.ds.copy(deep=True)\n",
    "spoiled_quantile = qplad_model.ds.copy(deep=True)\n",
    "\n",
    "# Spoil one dayoftheyear value in adjustment factors (force it to be 0.0)\n",
    "# and test that the spoiled value correctly propigates through to output.\n",
    "time_idx_to_spoil = 25\n",
    "spoiled_time[\"af\"][:, time_idx_to_spoil, :] = 0.0\n",
    "qplad_model.ds = spoiled_time\n",
    "downscaled = adjust_analogdownscaling(\n",
    "    simulation=biascorrected_fine.set_coords(\n",
    "        [\"sim_q\"]\n",
    "    ),  # func assumes sim_q is coordinate...\n",
    "    qplad=qplad_model,\n",
    "    variable=variable,\n",
    ")\n",
    "\n",
    "# All but two values should be 1.0...\n",
    "assert (downscaled[variable].values == 1.0).sum() == 728\n",
    "# We should have 2 `0.0` entires. One in each lat...\n",
    "assert (downscaled[variable].values == 0.0).sum() == 2\n",
    "# All our 0.0s should be in this dayofyear/time slice in output dataset.\n",
    "np.testing.assert_array_equal(\n",
    "    downscaled[variable].values[time_idx_to_spoil, :], np.array([0.0, 0.0])\n",
    ")\n",
    "\n",
    "# Similar to above, spoil one lat value in adjustment factors\n",
    "# (force it to be 0.0) and test that the spoiled value correctly\n",
    "# propagates through to output.\n",
    "latitude_idx_to_spoil = 0\n",
    "spoiled_latitude[\"af\"][latitude_idx_to_spoil, ...] = 0.0\n",
    "qplad_model.ds = spoiled_latitude\n",
    "downscaled = adjust_analogdownscaling(\n",
    "    simulation=biascorrected_fine.set_coords(\n",
    "        [\"sim_q\"]\n",
    "    ),  # func assumes sim_q is coordinate...\n",
    "    qplad=qplad_model,\n",
    "    variable=variable,\n",
    ")\n",
    "# Half of values in output should be 1.0...\n",
    "assert (downscaled[variable].values == 1.0).sum() == 365\n",
    "# The other half should be `0.0` due to the spoiled data...\n",
    "assert (downscaled[variable].values == 0.0).sum() == 365\n",
    "# All our 0.0s should be in this single lat in output dataset.\n",
    "assert all(downscaled[variable].values[:, latitude_idx_to_spoil] == 0.0)\n",
    "\n",
    "# spoil one quantile in adjustment factors for one day of year\n",
    "# force it to be 200 and ensure that a bias corrected day with that\n",
    "# quantile gets the spoiled value after downscaling\n",
    "# pick a day of year\n",
    "doy = 100\n",
    "# only do this for one lat pt\n",
    "lat_pt = 0\n",
    "# get the quantile from the bias corrected data for this doy and latitude\n",
    "q_100 = biascorrected_fine.sim_q[lat_pt, doy]\n",
    "# extract quantiles from afs to get the corresponding quantile index\n",
    "bc_quantiles = qplad_model.ds.af[0, 100, :].quantiles.values\n",
    "# get index of the af for that day\n",
    "q_idx = np.argmin(np.abs(q_100 - bc_quantiles))\n",
    "\n",
    "# now spoil that doy quantile adjustment factor\n",
    "spoiled_quantile[\"af\"][0, 100, q_idx] = 200\n",
    "qplad_model.ds[\"af\"] = spoiled_quantile[\"af\"]\n",
    "\n",
    "downscaled = adjust_analogdownscaling(\n",
    "    simulation=biascorrected_fine.set_coords(\n",
    "        [\"sim_q\"]\n",
    "    ),  # func assumes sim_q is coordinate...\n",
    "    qplad=qplad_model,\n",
    "    variable=variable,\n",
    ")\n",
    "\n",
    "# the 100th doy and corresponding quantile should be equal to the spoiled value\n",
    "assert np.max(downscaled[variable].values[lat_pt, :]) == 200\n",
    "assert np.argmax(downscaled[variable].values[lat_pt, :]) == 100\n",
    "# check that the adjustment factor did not get applied to any other days of the year\n",
    "assert (downscaled[variable].values[lat_pt, :]).sum() == 564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
