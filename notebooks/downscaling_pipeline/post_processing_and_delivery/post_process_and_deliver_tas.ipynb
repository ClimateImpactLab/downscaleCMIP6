{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071c72b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Rechunk, flip, Q/A, and deliver tasmin & tasmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7390f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY = '''\n",
    "v1.1 : switch to additive QDM tasmin with swapping where tasmin > tasmax; also include\n",
    "       regridding with nearest neighbor patch in QPLAD.\n",
    "v1.0 : initial release; QDM tasmax (additive) and DTR (multiplicative)\n",
    "'''.strip()\n",
    "\n",
    "OUTPUT_VERSION = 'v1.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1d1c2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.8/site-packages/dask_gateway/client.py:21: FutureWarning: format_bytes is deprecated and will be removed in a future release. Please use dask.utils.format_bytes instead.\n",
      "  from distributed.utils import LoopRunner, format_bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import fsspec\n",
    "import requests\n",
    "import contextlib\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import zarr\n",
    "import rechunker\n",
    "import dask\n",
    "import rhg_compute_tools.kubernetes as rhgk\n",
    "import rhg_compute_tools.utils as rhgu\n",
    "import dask.distributed as dd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from yaml import load, Loader\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2162c6b0-1908-4902-98a0-83fc4612b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_prep as up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7354c4dd-6705-4a0a-9a07-84f5bee03c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIVERY_MODELS = up.delivery_models()\n",
    "INSTITUTIONS = up.institutions()\n",
    "ENSEMBLE_MEMBERS = up.ensemble_members_tas()\n",
    "GRID_SPECS = up.grid_specs_tas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b450d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscaled_filepatt = (\n",
    "    'gs://downscaled-288ec5ac/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}.zarr'\n",
    ")\n",
    "\n",
    "rechunked_temp_store_pattern = (\n",
    "    'gs://scratch-170cd6ec/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}-rechunked-temp-store.zarr'\n",
    ")\n",
    "\n",
    "rechunked_pattern = (\n",
    "    'gs://scratch-170cd6ec/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}-rechunked.zarr'\n",
    ")\n",
    "\n",
    "fipped_pattern = (\n",
    "    'gs://scratch-170cd6ec/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}-tasminmax-flipped.zarr'\n",
    ")\n",
    "\n",
    "OUTPUT_PATTERN = (\n",
    "    'gs://downscaled-288ec5ac/outputs/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{delivery_version}.zarr'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c4dd488-d099-4af4-8e88-34bd0dc000d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('gs', timeout=120, cache_timeout=120, requests_timeout=120, read_timeout=120, conn_timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b97757a8-71d1-4c8b-b75e-108224e610cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec382b53aed4551b64812578e4ac6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tasmax:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11570/2409927811.py:21: UserWarning: skipping ACCESS-ESM1-5-ssp585 tasmax as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmax as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:21: UserWarning: skipping ACCESS-CM2-ssp126 tasmax as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmax as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:21: UserWarning: skipping ACCESS-CM2-ssp585 tasmax as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmax as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:21: UserWarning: skipping GFDL-CM4-ssp126 tasmax as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmax as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:21: UserWarning: skipping GFDL-CM4-ssp370 tasmax as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmax as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:21: UserWarning: skipping NESM3-ssp370 tasmax as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmax as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:21: UserWarning: skipping MPI-ESM1-2-HR-ssp245 tasmax as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmax as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:21: UserWarning: skipping MPI-ESM1-2-HR-ssp370 tasmax as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmax as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:21: UserWarning: skipping EC-Earth3-AerChem-ssp126 tasmax as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmax as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:21: UserWarning: skipping EC-Earth3-AerChem-ssp245 tasmax as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmax as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:21: UserWarning: skipping EC-Earth3-AerChem-ssp585 tasmax as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmax as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:21: UserWarning: skipping EC-Earth3-CC-ssp126 tasmax as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmax as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:21: UserWarning: skipping EC-Earth3-CC-ssp370 tasmax as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmax as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:21: UserWarning: skipping HadGEM3-GC31-LL-ssp370 tasmax as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmax as I did not find it in the all paths yaml file')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14067aed22d242a8990caf2df27974b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tasmin:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11570/2409927811.py:47: UserWarning: skipping ACCESS-ESM1-5-ssp585 tasmin as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmin as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:47: UserWarning: skipping ACCESS-CM2-ssp126 tasmin as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmin as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:47: UserWarning: skipping ACCESS-CM2-ssp585 tasmin as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmin as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:47: UserWarning: skipping GFDL-CM4-ssp126 tasmin as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmin as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:47: UserWarning: skipping GFDL-CM4-ssp370 tasmin as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmin as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:47: UserWarning: skipping NESM3-ssp370 tasmin as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmin as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:47: UserWarning: skipping MPI-ESM1-2-HR-ssp245 tasmin as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmin as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:47: UserWarning: skipping MPI-ESM1-2-HR-ssp370 tasmin as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmin as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:47: UserWarning: skipping EC-Earth3-AerChem-ssp126 tasmin as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmin as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:47: UserWarning: skipping EC-Earth3-AerChem-ssp245 tasmin as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmin as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:47: UserWarning: skipping EC-Earth3-AerChem-ssp585 tasmin as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmin as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:47: UserWarning: skipping EC-Earth3-CC-ssp126 tasmin as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmin as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:47: UserWarning: skipping EC-Earth3-CC-ssp370 tasmin as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmin as I did not find it in the all paths yaml file')\n",
      "/tmp/ipykernel_11570/2409927811.py:47: UserWarning: skipping HadGEM3-GC31-LL-ssp370 tasmin as I did not find it in the all paths yaml file\n",
      "  warnings.warn(f'skipping {m}-{scen} tasmin as I did not find it in the all paths yaml file')\n"
     ]
    }
   ],
   "source": [
    "with open(f'data_paths.yaml') as infile:\n",
    "    allpaths = load(infile, Loader)\n",
    "tasmax_fps = {m: {} for m in DELIVERY_MODELS}\n",
    "\n",
    "for m in tqdm(DELIVERY_MODELS, desc='tasmax'):\n",
    "\n",
    "    inst = INSTITUTIONS[m]\n",
    "\n",
    "    for act, scen in [\n",
    "        ('CMIP', 'historical'),\n",
    "        ('ScenarioMIP', 'ssp126'),\n",
    "        ('ScenarioMIP', 'ssp245'),\n",
    "        ('ScenarioMIP', 'ssp370'),\n",
    "        ('ScenarioMIP', 'ssp585'),\n",
    "    ]:\n",
    "        if m == 'MPI-ESM1-2-HR' and scen == 'historical':\n",
    "            inst = 'MPI-M'\n",
    "        if scen in allpaths[f'{m}-tasmax']:\n",
    "            tasmax_fps[m][scen] = allpaths[f'{m}-tasmax'][scen]['downscaled']\n",
    "        else:\n",
    "            warnings.warn(f'skipping {m}-{scen} tasmax as I did not find it in the all paths yaml file')\n",
    "\n",
    "\n",
    "tasmax_max_versions = {\n",
    "    m: {s: max(vs) for s, vs in mspec.items() if len(vs) > 0}\n",
    "    for m, mspec in tasmax_fps.items()\n",
    "}\n",
    "\n",
    "tasmin_fps = {m: {} for m in DELIVERY_MODELS}\n",
    "\n",
    "for m in tqdm(DELIVERY_MODELS, desc='tasmin'):\n",
    "\n",
    "    inst = INSTITUTIONS[m]\n",
    "\n",
    "    for act, scen in [\n",
    "        ('CMIP', 'historical'),\n",
    "        ('ScenarioMIP', 'ssp126'),\n",
    "        ('ScenarioMIP', 'ssp245'),\n",
    "        ('ScenarioMIP', 'ssp370'),\n",
    "        ('ScenarioMIP', 'ssp585'),\n",
    "    ]:\n",
    "        if m == 'MPI-ESM1-2-HR' and scen == 'historical':\n",
    "            inst = 'MPI-M'\n",
    "        if scen in allpaths[f'{m}-tasmin']:\n",
    "            tasmin_fps[m][scen] = allpaths[f'{m}-tasmin'][scen]['downscaled']\n",
    "        else:\n",
    "            warnings.warn(f'skipping {m}-{scen} tasmin as I did not find it in the all paths yaml file')\n",
    "\n",
    "\n",
    "tasmin_max_versions = {\n",
    "    m: {s: max(vs) for s, vs in mspec.items() if len(vs) > 0}\n",
    "    for m, mspec in tasmin_fps.items()\n",
    "}\n",
    "\n",
    "INPUT_FILE_VERSIONS = {\n",
    "    'version': OUTPUT_VERSION,\n",
    "    'created': pd.Timestamp.now(tz='US/Pacific').strftime('%c'),\n",
    "    'history': HISTORY,\n",
    "    'file_paths': {\n",
    "        'tasmin': tasmin_fps,\n",
    "        'tasmax': tasmax_fps,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "576c40d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CC0_LICENSE_MODELS, CC_BY_LICENSE_MODELS, CC_BY_SA_LICENSE_MODELS = up.licenses()\n",
    "up.check_licenses(delivery_models=DELIVERY_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1481f542",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d823abe",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7e80b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=[\n",
    "    'downscaled_filepatt',\n",
    "    'rechunked_temp_store_pattern',\n",
    "    'rechunked_pattern',\n",
    "    'fipped_pattern',\n",
    "    'OUTPUT_PATTERN',\n",
    "])\n",
    "def get_spec_from_input_fp(fp, output_version=OUTPUT_VERSION):\n",
    "    (\n",
    "        bucket,\n",
    "        stage,\n",
    "        activity,\n",
    "        institution,\n",
    "        model,\n",
    "        scenario,\n",
    "        ensemble,\n",
    "        table,\n",
    "        variable,\n",
    "        grid,\n",
    "        run_version,\n",
    "    ) = os.path.splitext(fp)[0].replace('gs://', '').split('/')\n",
    "\n",
    "    spec = dict(\n",
    "        bucket=bucket,\n",
    "        stage=stage,\n",
    "        activity=activity,\n",
    "        institution=institution,\n",
    "        model=model,\n",
    "        scenario=scenario,\n",
    "        ensemble=ensemble,\n",
    "        table=table,\n",
    "        variable=variable,\n",
    "        grid=grid,\n",
    "        run_version=run_version,\n",
    "    )\n",
    "\n",
    "    for (name, fpatt) in [\n",
    "        ('downscaled_fp', downscaled_filepatt),\n",
    "        ('output_fp', OUTPUT_PATTERN),\n",
    "    ]:\n",
    "        spec[name] = fpatt.format(\n",
    "            activity_id=activity,\n",
    "            institution_id=institution,\n",
    "            source_id=model,\n",
    "            experiment_id=scenario,\n",
    "            member_id=ensemble,\n",
    "            variable_id=variable,\n",
    "            table_id=table,\n",
    "            grid_spec=grid,\n",
    "            run_version=run_version,\n",
    "            delivery_version=output_version,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    for (name, fpatt) in [\n",
    "        ('rechunk_temp_store_fp', rechunked_temp_store_pattern),\n",
    "        ('rechunked_fp', rechunked_pattern),\n",
    "        ('flipped_fp', fipped_pattern),\n",
    "    ]:\n",
    "        spec[name] = fpatt.format(\n",
    "            activity_id=activity,\n",
    "            institution_id=institution,\n",
    "            source_id=model,\n",
    "            experiment_id=scenario,\n",
    "            member_id=ensemble,\n",
    "            variable_id=variable,\n",
    "            table_id=table,\n",
    "            grid_spec=grid,\n",
    "            run_version = up.datetime_version(),\n",
    "            delivery_version=output_version,\n",
    "        )\n",
    "\n",
    "    return spec\n",
    "\n",
    "@rhgu.block_globals\n",
    "def get_spec_from_output_fp(fp, output_pattern=OUTPUT_PATTERN):\n",
    "    \n",
    "    (\n",
    "        bucket,\n",
    "        stage,\n",
    "        activity,\n",
    "        institution,\n",
    "        model,\n",
    "        scenario,\n",
    "        ensemble,\n",
    "        table,\n",
    "        variable,\n",
    "        output_version,\n",
    "    ) = os.path.splitext(fp)[0].replace('gs://', '').split('/')\n",
    "\n",
    "    output_fp = output_pattern.format(\n",
    "        activity_id=activity,\n",
    "        institution_id=institution,\n",
    "        source_id=model,\n",
    "        experiment_id=scenario,\n",
    "        member_id=ensemble,\n",
    "        variable_id=variable,\n",
    "        table_id=table,\n",
    "        delivery_version=output_version,\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "        activity=activity,\n",
    "        institution=institution,\n",
    "        model=model,\n",
    "        scenario=scenario,\n",
    "        ensemble=ensemble,\n",
    "        table=table,\n",
    "        variable=variable,\n",
    "        output_version=output_version,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7ebde",
   "metadata": {},
   "source": [
    "## Stage 1: Rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2800b087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=[\n",
    "    'INPUT_FILE_VERSIONS',\n",
    "])\n",
    "def rechunk_data(varname, model, scenario, worker_memory_limit):\n",
    "\n",
    "    fs = fsspec.filesystem('gs', timeout=120, cache_timeout=120, requests_timeout=120, read_timeout=120, conn_timeout=120)\n",
    "\n",
    "    target_chunks = {\n",
    "        varname: {'time': 365, 'lat': 360, 'lon': 360},\n",
    "        'time': {'time': 365},\n",
    "        'lat': {'lat': 360},\n",
    "        'lon': {'lon': 360},\n",
    "    }\n",
    "\n",
    "    input_fp = INPUT_FILE_VERSIONS['file_paths'][varname][model][scenario]\n",
    "    input_spec = get_spec_from_input_fp(input_fp)\n",
    "\n",
    "    rechunked_temp_store_fp = input_spec['rechunk_temp_store_fp']\n",
    "    rechunked_fp = input_spec['rechunked_fp']\n",
    "\n",
    "    mapper = fs.get_mapper(input_fp)\n",
    "    with xr.open_zarr(mapper) as ds:\n",
    "\n",
    "        rechunked_mapper = fs.get_mapper(rechunked_fp)\n",
    "\n",
    "        chunk_job = rechunker.rechunk(\n",
    "            source=ds,\n",
    "            target_chunks=target_chunks,\n",
    "            max_mem=worker_memory_limit,\n",
    "            target_store=rechunked_mapper,\n",
    "            temp_store=fs.get_mapper(rechunked_temp_store_fp),\n",
    "        )\n",
    "\n",
    "        chunk_job_persist = chunk_job._plan.persist()\n",
    "        dd.wait(chunk_job_persist)\n",
    "\n",
    "    zarr.convenience.consolidate_metadata(rechunked_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab00a4",
   "metadata": {},
   "source": [
    "## Stage 2: Flip Negative DTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2971d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=['INPUT_FILE_VERSIONS', 'rechunked_pattern', 'fipped_pattern'])\n",
    "def flip_negative_dtr(model, scenario):\n",
    "\n",
    "    fs = fsspec.filesystem('gs', timeout=120, cache_timeout=120, requests_timeout=120, read_timeout=120, conn_timeout=120)\n",
    "    client = dd.get_client()\n",
    "\n",
    "    tasmin_input_fp = INPUT_FILE_VERSIONS['file_paths']['tasmin'][model][scenario]\n",
    "    tasmin_input_spec = get_spec_from_input_fp(tasmin_input_fp)\n",
    "    tasmin_rechunked_fp = tasmin_input_spec['rechunked_fp']\n",
    "    tasmin_fipped_fp = tasmin_input_spec['flipped_fp']\n",
    "\n",
    "    tasmax_input_fp = INPUT_FILE_VERSIONS['file_paths']['tasmax'][model][scenario]\n",
    "    tasmax_input_spec = get_spec_from_input_fp(tasmax_input_fp)\n",
    "    tasmax_rechunked_fp = tasmax_input_spec['rechunked_fp']\n",
    "    tasmax_fipped_fp = tasmax_input_spec['flipped_fp']\n",
    "\n",
    "    tasmin_in_mapper = fs.get_mapper(tasmin_rechunked_fp)\n",
    "    tasmax_in_mapper = fs.get_mapper(tasmax_rechunked_fp)\n",
    "\n",
    "    with xr.open_zarr(tasmin_in_mapper) as tasmin_ds, xr.open_zarr(tasmax_in_mapper) as tasmax_ds:\n",
    "\n",
    "        tasmin_ds_out = tasmin_ds.copy(deep=False)\n",
    "        tasmax_ds_out = tasmax_ds.copy(deep=False)\n",
    "\n",
    "        tasmin_ds_out['tasmin'] = np.minimum(tasmin_ds['tasmin'], tasmax_ds['tasmax'])\n",
    "        tasmax_ds_out['tasmax'] = np.maximum(tasmin_ds['tasmin'], tasmax_ds['tasmax'])\n",
    "\n",
    "        tasmin_ds_out['tasmin'].attrs.update(tasmin_ds['tasmin'].attrs)\n",
    "        tasmax_ds_out['tasmax'].attrs.update(tasmax_ds['tasmax'].attrs)\n",
    "\n",
    "        dtr = (tasmax_ds_out['tasmax'] - tasmin_ds_out['tasmin'])\n",
    "        min_dtr = dtr.min()\n",
    "        dtr_usually_positive = (dtr > 0.1).mean()\n",
    "\n",
    "        tasmin_out_mapper = fs.get_mapper(tasmin_fipped_fp)\n",
    "        tasmax_out_mapper = fs.get_mapper(tasmax_fipped_fp)\n",
    "\n",
    "        write_tasmin = tasmin_ds_out.to_zarr(tasmin_out_mapper, consolidated=True, compute=False)\n",
    "        write_tasmax = tasmax_ds_out.to_zarr(tasmax_out_mapper, consolidated=True, compute=False)\n",
    "\n",
    "        min_dtr, dtr_usually_positive, write_tasmin, write_tasmax = client.compute( \n",
    "            [min_dtr, dtr_usually_positive, write_tasmin, write_tasmax],\n",
    "            optimize_graph=True,\n",
    "            retries=3,\n",
    "            sync=True,\n",
    "        )\n",
    "\n",
    "        assert (min_dtr >= 0).item() is True, (\n",
    "            f\"DTR not always positive after flip: min DTR: {min_dtr.item()}\"\n",
    "        )\n",
    "\n",
    "        if (dtr_usually_positive > 0.99).item() is not True:\n",
    "            warnings.warn(f\"DTR not almost always > 0.1: {dtr_usually_positive.item()}\")\n",
    "#         assert (dtr_usually_positive > 0.99).item() is True, (\n",
    "#             f\"DTR not almost always > 0.1: {dtr_usually_positive.item()}\"\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968ba04e",
   "metadata": {},
   "source": [
    "## Stage 3: copy to destination directory & validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6d460f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=['CC0_LICENSE_MODELS', 'CC_BY_LICENSE_MODELS', 'CC_BY_SA_LICENSE_MODELS'])\n",
    "def quick_check_file(fp, ds, spec):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # check that metadata matches file spec\n",
    "\n",
    "    assert ds.attrs['institution_id'] == spec['institution'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['institution_id']} ≠ {spec['institution']}\"\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['source_id'] == spec['model'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['source_id']} ≠ {spec['model']}\"\n",
    "    )\n",
    "\n",
    "    assert spec['activity'] in ds.attrs['activity_id'], (\n",
    "        f\"invalid attrs in {fp}: {spec['activity']} not in {ds.attrs['activity_id']}\"\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['experiment_id'] == spec['scenario'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['experiment_id']} ≠ {spec['scenario']}\"\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['variant_label'] == spec['ensemble'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['variant_label']} ≠ {spec['ensemble']}\"\n",
    "    )\n",
    "\n",
    "    if spec['variable'] == 'tasmax':\n",
    "        assert ds['tasmax'].attrs['long_name'] == 'Daily Maximum Near-Surface Air Temperature'\n",
    "        assert ds['tasmax'].attrs['units'] == 'K'\n",
    "    elif spec['variable'] == 'tasmin':\n",
    "        assert ds['tasmin'].attrs['long_name'] == 'Daily Minimum Near-Surface Air Temperature'\n",
    "        assert ds['tasmin'].attrs['units'] == 'K'\n",
    "    elif spec['variable'] == 'pr':\n",
    "        raise NotImplementedError()\n",
    "#         assert ds['tasmax'].attrs['units'] == 'mm/day'\n",
    "    else:\n",
    "        raise ValueError(f'variable not recognized: {spec[\"variable\"]}')\n",
    "\n",
    "    # Check licensing fields & endpoint URL\n",
    "\n",
    "    # check that license URL points to a real location and it exists\n",
    "    license_url = ds.attrs['license']\n",
    "    assert ds.attrs['source_id'] in license_url, (\n",
    "        f'model \"{ds.attrs[\"source_id\"]}\" not found in license url: {license_url}'\n",
    "    )\n",
    "    r = requests.get(license_url)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # check that \"Creaive Commons\" and the model name show up on the page\n",
    "    assert ds.attrs['source_id'] in r.text, (\n",
    "        f'model \"{ds.attrs[\"source_id\"]}\" not found on license page: {license_url}'\n",
    "    )\n",
    "\n",
    "    assert \"Creative Commons\" in r.text, (\n",
    "        f'\"Creative Commons\" not found on license page: {license_url}'\n",
    "    )\n",
    "\n",
    "    # check that \"Creative Commons\" appears in the raw license text\n",
    "\n",
    "    raw_license_url = (\n",
    "        ds.attrs['license']\n",
    "        .replace('github.com', 'raw.githubusercontent.com')\n",
    "        .replace('/blob/', '/')\n",
    "        .replace('/tree/', '/')\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['source_id'] in raw_license_url, (\n",
    "        f'model \"{ds.attrs[\"source_id\"]}\" not found in license url: {raw_license_url}'\n",
    "    )\n",
    "    r = requests.get(raw_license_url)\n",
    "    r.raise_for_status()\n",
    "    assert 'Creative Commons' in r.text, (\n",
    "        f'\"Creative Commons\" not found in license text: {raw_license_url}'\n",
    "    )\n",
    "\n",
    "    if spec['model'] in CC0_LICENSE_MODELS:\n",
    "        assert 'CC0 1.0 Universal' in r.text, (\n",
    "            f\"expected CC0 license for {spec['model']} at {fp}\"\n",
    "        )\n",
    "    elif spec['model'] in CC_BY_LICENSE_MODELS:\n",
    "        assert 'Attribution 4.0 International' in r.text, (\n",
    "            f\"expected CC-BY 4.0 license for {spec['model']} at {fp}\"\n",
    "        )\n",
    "    elif spec['model'] in CC_BY_SA_LICENSE_MODELS:\n",
    "        assert 'Attribution-ShareAlike 4.0 International' in r.text, (\n",
    "            f\"expected CC-BY-SA 4.0 license for {spec['model']} at {fp}\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"deploying model with unknown license: {spec['model']} at {fp}\"\n",
    "        )\n",
    "\n",
    "    # Check dimension size & membership\n",
    "\n",
    "    for c in ds.coords.keys():\n",
    "        assert ds.coords[c].notnull().all().item() is True, f\"NaNs found in coordinate '{c}' in {fp}\"\n",
    "\n",
    "    if spec['activity'] == 'ScenarioMIP':\n",
    "        date_range = xr.cftime_range(\"2015-01-01\", \"2099-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "        if len(ds.time) > len(date_range):\n",
    "            date_range = xr.cftime_range(\"2015-01-01\", \"2100-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "    else:\n",
    "        date_range = xr.cftime_range(\"1950-01-01\", \"2014-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "\n",
    "    assert ds.sizes['time'] == len(date_range), (\n",
    "        f\"unexpected length of dimension 'time': length {len(ds.time)}; \"\n",
    "        f\"expected {len(date_range)} in {fp}\"\n",
    "    )\n",
    "\n",
    "    assert date_range.isin(ds.time.dt.floor('D').values).all(), f\"invalid coords in {fp}\"\n",
    "\n",
    "    assert pd.Series(np.arange(-179.875, 180, 0.25)).isin(ds.lon.values).all(), (\n",
    "        f\"invalid coords in {fp}\"\n",
    "    )\n",
    "    assert pd.Series(np.arange(-89.875, 90, 0.25)).isin(ds.lat.values).all(), (\n",
    "        f\"invalid coords in {fp}\"\n",
    "    )\n",
    "\n",
    "    varnames = list(ds.data_vars.keys())\n",
    "    assert len(varnames) == 1\n",
    "    varname = varnames[0]\n",
    "\n",
    "    assert ds[varname].sizes['lat'] == 720, f\"lat not length 720 in {fp}:\\n{ds}\"\n",
    "    assert ds[varname].sizes['lon'] == 1440, f\"lon not length 1440 in {fp}:\\n{ds}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3fddb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=['INPUT_FILE_VERSIONS'])\n",
    "def validate_outputs(fp, quick=False, check_dtr=False):\n",
    "    spec = get_spec_from_output_fp(fp)\n",
    "\n",
    "    fs = fsspec.filesystem('gs', timeout=120, cache_timeout=120, requests_timeout=120, read_timeout=120, conn_timeout=120)\n",
    "\n",
    "    mapper = fs.get_mapper(fp)\n",
    "\n",
    "    if check_dtr and (spec['variable'] != 'tasmin'):\n",
    "        raise ValueError('check_dtr can only be used with variable == \"tasmin\"')\n",
    "\n",
    "    if check_dtr:\n",
    "        tasmax_spec = get_spec_from_input_fp(\n",
    "            INPUT_FILE_VERSIONS['file_paths']['tasmax'][spec['model']][spec['scenario']]\n",
    "        )\n",
    "\n",
    "        tasmax_fp = tasmax_spec['output_fp']\n",
    "        tasmax_mapper = fs.get_mapper(tasmax_fp)\n",
    "        tasmax_opener = xr.open_zarr(tasmax_mapper)\n",
    "\n",
    "    else:\n",
    "        tasmax_opener = contextlib.nullcontext()\n",
    "\n",
    "    with xr.open_zarr(mapper) as ds, tasmax_opener as tasmax_ds:\n",
    "\n",
    "        quick_check_file(fp, ds, spec)\n",
    "\n",
    "        if quick:\n",
    "            return\n",
    "\n",
    "        # check variable contents\n",
    "\n",
    "        varnames = list(ds.data_vars.keys())\n",
    "        assert len(varnames) == 1\n",
    "        varname = varnames[0]\n",
    "\n",
    "        to_check = ds[varname].sel(lat=slice(-80, 80))\n",
    "\n",
    "        nans = to_check.isnull().any()\n",
    "        min_val = to_check.min()\n",
    "        max_val = to_check.max()\n",
    "\n",
    "        if check_dtr:\n",
    "            assert varname == 'tasmin'\n",
    "            min_dtr = (tasmax_ds.tasmax.sel(lat=slice(-80, 80)) - to_check).min()\n",
    "        else:\n",
    "            min_dtr = dask.delayed(lambda: None)()\n",
    "\n",
    "        nans, vmin, vmax, min_dtr = dd.get_client().compute(\n",
    "            [nans, min_val, max_val, min_dtr],\n",
    "            optimize_graph=True,\n",
    "            sync=True,\n",
    "            retries=3,\n",
    "        )\n",
    "\n",
    "        assert nans.item() is False, f\"NaNs found in {fp}\"\n",
    "\n",
    "        if varname == 'tasmax':\n",
    "            allowed_min = 130\n",
    "            allowed_max = 377\n",
    "        elif varname == 'tasmin':\n",
    "            allowed_min = 130\n",
    "            allowed_max = 377\n",
    "        elif varname == 'pr':\n",
    "            allowed_min = 0\n",
    "            allowed_max = 3000\n",
    "        else:\n",
    "            raise ValueError(f'Variable name not recognized: {varname}\\nin file: {fp}')\n",
    "\n",
    "        import warnings \n",
    "        \n",
    "#         if (vmin >= allowed_min).item() is not True:\n",
    "#             warnings.warn(\n",
    "#                 f\"min value {vmin} outside allowed range [{allowed_min}, {allowed_max}]\"\n",
    "#             )\n",
    "            \n",
    "#         if (vmax <= allowed_max).item() is not True:\n",
    "#             warnings.warn(\n",
    "#                 f\"max value {vmax} outside allowed range [{allowed_min}, {allowed_max}]\"\n",
    "#             )\n",
    "            \n",
    "#         if check_dtr:\n",
    "#             if (min_dtr >= 0).item() is not True:\n",
    "#                 warnings.warn(\n",
    "#                     f\"DTR not greater than zero - min DTR: {min_dtr.item()} in {fp}\"\n",
    "#                 )\n",
    "\n",
    "        assert (vmin >= allowed_min).item() is True, (\n",
    "            f\"min value {vmin} outside allowed range [{allowed_min}, {allowed_max}] \"\n",
    "            f\"for {varname} in {fp}\"\n",
    "        )\n",
    "        assert (vmax <= allowed_max).item() is True, (\n",
    "            f\"max value {vmax} outside allowed range [{allowed_min}, {allowed_max}] \"\n",
    "            f\"for {varname} in {fp}\"\n",
    "        )\n",
    "\n",
    "        if check_dtr:\n",
    "            assert (min_dtr >= 0).item() is True, (\n",
    "                f\"DTR not greater than zero - min DTR: {min_dtr.item()} in {fp}\"\n",
    "            )\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def copy_and_validate(\n",
    "    source_fp,\n",
    "    output_version=OUTPUT_VERSION,\n",
    "    check=False,\n",
    "    deep_copy_check=False,\n",
    "    quick_check_and_retry=True,\n",
    "    overwrite=False,\n",
    "    overwrite_on_failure=False,\n",
    "    check_dtr=False,\n",
    "    pbar=False,\n",
    "):\n",
    "\n",
    "    spec = get_spec_from_input_fp(source_fp, output_version=output_version)\n",
    "    flipped_fp = spec['flipped_fp']\n",
    "    output_fp = spec['output_fp']\n",
    "    model = spec['model']\n",
    "    scenario = spec['scenario']\n",
    "\n",
    "    fs = fsspec.filesystem('gs', timeout=120, cache_timeout=120, requests_timeout=120, read_timeout=120, conn_timeout=120)\n",
    "\n",
    "    if fs.exists(output_fp):\n",
    "        if overwrite:\n",
    "            fs.remove(output_fp, recursive=True)\n",
    "\n",
    "        else:\n",
    "            if deep_copy_check:\n",
    "                dirs = list([(d, f) for d, dirs, fps in fs.walk(flipped_fp) for f in fps])\n",
    "                if pbar:\n",
    "                    dirs = tqdm(dirs)\n",
    "\n",
    "                for d, f in dirs:\n",
    "                    src = flipped_fp[:5] + os.path.join(d, f)\n",
    "                    dst = os.path.join(output_fp, os.path.relpath(src, flipped_fp))\n",
    "                    assert '..' not in dst\n",
    "                    src_hash = fs.stat(src)['md5Hash']\n",
    "\n",
    "                    for i in range(5):\n",
    "                        try:\n",
    "                            assert (src_hash == fs.stat(dst)['md5Hash'])\n",
    "                            break\n",
    "                        except (FileNotFoundError, AssertionError):\n",
    "                            if i == 4:\n",
    "                                raise\n",
    "\n",
    "                            fs.rm(dst)\n",
    "                            fs.copy(src, dst)\n",
    "\n",
    "            if check:\n",
    "                try:\n",
    "                    validate_outputs(\n",
    "                        output_fp,\n",
    "                        check_dtr=(check_dtr and (spec['variable'] == 'tasmin')),\n",
    "                    )\n",
    "                    return\n",
    "                except (\n",
    "                    AssertionError,\n",
    "                    FileNotFoundError,\n",
    "                    ValueError,\n",
    "                    IOError,\n",
    "                    xr.coding.times.OutOfBoundsDatetime,\n",
    "                    OverflowError,\n",
    "                ):\n",
    "                    if overwrite_on_failure:\n",
    "                        import warnings\n",
    "                        warnings.warn(f'validation failed for {output_fp}, deleting file')\n",
    "                        fs.rm(output_fp, recursive=True)\n",
    "                        return\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            elif quick_check_and_retry:\n",
    "                try:\n",
    "                    validate_outputs(output_fp, quick=True)\n",
    "                    return\n",
    "                except (\n",
    "                    OverflowError,\n",
    "                    IOError,\n",
    "                    zarr.errors.GroupNotFoundError,\n",
    "                    FileNotFoundError,\n",
    "                    AssertionError,\n",
    "                    ValueError,\n",
    "                ):\n",
    "                    pass\n",
    "\n",
    "                fs.rm(output_fp, recursive=True)\n",
    "            else:\n",
    "                return\n",
    "\n",
    "    print(f'copying:\\n\\tsrc:\\t{flipped_fp}\\n\\tdst:\\t{output_fp}')\n",
    "    fs.copy(flipped_fp, output_fp, recursive=True, batch_size=1000)\n",
    "\n",
    "    if deep_copy_check:\n",
    "        for d, f in list([(d, f) for d, dirs, fps in fs.walk(flipped_fp) for f in fps]):\n",
    "            src = flipped_fp[:5] + os.path.join(d, f)\n",
    "            dst = os.path.join(output_fp, os.path.relpath(src, flipped_fp))\n",
    "            assert '..' not in dst\n",
    "            src_hash = fs.stat(src)['md5Hash']\n",
    "\n",
    "            for i in range(5):\n",
    "                try:\n",
    "                    assert (src_hash == fs.stat(dst)['md5Hash'])\n",
    "                    break\n",
    "                except (FileNotFoundError, AssertionError):\n",
    "                    if i == 4:\n",
    "                        raise\n",
    "\n",
    "                    fs.rm(dst)\n",
    "                    fs.copy(src, dst)\n",
    "\n",
    "    if check:\n",
    "        validate_outputs(\n",
    "            output_fp,\n",
    "            check_dtr=(check_dtr and (spec['variable'] == 'tasmin')),\n",
    "        )\n",
    "    elif quick_check_and_retry:\n",
    "        validate_outputs(output_fp, quick=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb3634",
   "metadata": {},
   "source": [
    "# Full workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "de5a96ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0934871980d41609505fc7ce2936dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>GatewayCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n<style scoped>\\n    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client, cluster = rhgk.get_giant_cluster()\n",
    "N_WORKERS = 60\n",
    "cluster.scale(N_WORKERS)\n",
    "\n",
    "MAX_MEM = '16GB' # for standard cluster\n",
    "\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "526d3a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://jhub-dev.onyx.climateriskservice.com/services/dask-gateway/clusters/jhub.6d97da2e64f14bda90059b9605a058f4/status\n"
     ]
    }
   ],
   "source": [
    "print('https://jhub-dev.onyx.climateriskservice.com' + cluster.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "dcb313f2-33c1-49a2-94f0-4442f1ed67c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wait until the workers come online - otherwise we get unbalanced task loading and things start acting all fishy\n",
    "import time\n",
    "while len(client.ncores()) < N_WORKERS:\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c369d1",
   "metadata": {},
   "source": [
    "# Prepare final outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "23d8baef-1275-4709-8c9d-59151d7c1fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('gs', timeout=120, cache_timeout=120, requests_timeout=120, read_timeout=120, conn_timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2fc1372c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1e14b7f52744ab83ceb694ab384ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://scratch-170cd6ec/stage/ScenarioMIP/MIROC/MIROC6/ssp585/r1i1p1f1/day/tasmin/gn/v20220125010223-rechunked.zarr\n",
      "gs://scratch-170cd6ec/stage/ScenarioMIP/MIROC/MIROC6/ssp585/r1i1p1f1/day/tasmax/gn/v20220125010352-rechunked.zarr\n",
      "gs://scratch-170cd6ec/stage/ScenarioMIP/MIROC/MIROC6/ssp585/r1i1p1f1/day/tasmin/gn/v20220125010223-tasminmax-flipped.zarr\n",
      "gs://scratch-170cd6ec/stage/ScenarioMIP/MIROC/MIROC6/ssp585/r1i1p1f1/day/tasmax/gn/v20220125010352-tasminmax-flipped.zarr\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with tqdm(DELIVERY_MODELS) as pbar:\n",
    "        for model in pbar:\n",
    "            for scenario in INPUT_FILE_VERSIONS['file_paths']['tasmin'][model].keys():\n",
    "\n",
    "                tasmin_input_fp = INPUT_FILE_VERSIONS['file_paths']['tasmin'][model][scenario]\n",
    "                tasmin_spec = get_spec_from_input_fp(tasmin_input_fp)\n",
    "\n",
    "                tasmax_input_fp = INPUT_FILE_VERSIONS['file_paths']['tasmax'][model][scenario]\n",
    "                tasmax_spec = get_spec_from_input_fp(tasmax_input_fp)\n",
    "                \n",
    "                #comment out this block to reproduce rechunked/flipped data on scratch bucket\n",
    "                if fs.exists(tasmin_spec['output_fp']) and fs.exists(tasmax_spec['output_fp']):\n",
    "                    print(f'skipping {model} {scenario} - output already exists')\n",
    "                    continue\n",
    "\n",
    "                pbar.set_postfix({'model': model, 'scen': scenario, 'stage': 'rechunk tasmin'})\n",
    "                rechunk_data('tasmin', model, scenario, worker_memory_limit=MAX_MEM)\n",
    "                pbar.set_postfix({'model': model, 'scen': scenario, 'stage': 'rechunk tasmax'})\n",
    "                rechunk_data('tasmax', model, scenario, worker_memory_limit=MAX_MEM)\n",
    "                pbar.set_postfix({'model': model, 'scen': scenario, 'stage': 'flip negative DTR'})\n",
    "                flip_negative_dtr(model, scenario)\n",
    "except Exception:\n",
    "    client.restart()\n",
    "    cluster.close()\n",
    "    client.close()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af325628-9a89-4158-97b9-50deed28016d",
   "metadata": {},
   "source": [
    "fp = 'gs://scratch-170cd6ec/stage/ScenarioMIP/MIROC/MIROC6/ssp585/r1i1p1f1/day/tasmax/gn/v20220125010352-rechunked-temp-store.zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4edc1e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasmin_files = [fp for m, v in INPUT_FILE_VERSIONS['file_paths']['tasmin'].items() for s, fp in v.items()]\n",
    "tasmax_files = [fp for m, v in INPUT_FILE_VERSIONS['file_paths']['tasmax'].items() for s, fp in v.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "72b2953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blocking_pbar(futures):\n",
    "    status = {'error': 0, 'killed': 0, 'lost': 0}\n",
    "    with tqdm(dd.as_completed(futures), total=len(futures)) as pbar:\n",
    "        for f in pbar:\n",
    "            if f.status in status.keys():\n",
    "                status[f.status] += 1\n",
    "                pbar.set_postfix(status)\n",
    "\n",
    "    dd.wait(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f0552",
   "metadata": {},
   "source": [
    "# Copy files to final destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "ddd32f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e69bfb83064a7eb72edc0abf210226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    tasmax_futures = client.map(\n",
    "        copy_and_validate,\n",
    "        tasmax_files,\n",
    "        output_version=OUTPUT_VERSION,\n",
    "        check=False,\n",
    "        deep_copy_check=False,\n",
    "        quick_check_and_retry=True,\n",
    "        overwrite=False,\n",
    "        overwrite_on_failure=False,\n",
    "        check_dtr=False,\n",
    "        pbar=False\n",
    "    )\n",
    "\n",
    "    blocking_pbar(tasmax_futures)\n",
    "except Exception:\n",
    "    client.restart()\n",
    "    cluster.scale(0)\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "ff055ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f120963e05460986059919e721c790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    tasmin_futures = client.map(\n",
    "        copy_and_validate,\n",
    "        tasmin_files,\n",
    "        output_version=OUTPUT_VERSION,\n",
    "        check=False,\n",
    "        deep_copy_check=False,\n",
    "        quick_check_and_retry=True,\n",
    "        overwrite=False,\n",
    "        overwrite_on_failure=False,\n",
    "        check_dtr=False,\n",
    "        pbar=False\n",
    "    )\n",
    "\n",
    "    blocking_pbar(tasmin_futures)\n",
    "except Exception:\n",
    "    client.restart()\n",
    "    cluster.scale(0)\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f503b50",
   "metadata": {},
   "source": [
    "# Deep copy check\n",
    "Check every file against source to ensure a complete copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b8624956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b970a38027924647a33edc79196a4984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    tasmax_futures = client.map(\n",
    "        copy_and_validate,\n",
    "        tasmax_files,\n",
    "        output_version=OUTPUT_VERSION,\n",
    "        check=False,\n",
    "        deep_copy_check=True,\n",
    "        quick_check_and_retry=True,\n",
    "        overwrite=False,\n",
    "        overwrite_on_failure=False,\n",
    "        check_dtr=False,\n",
    "        pbar=False\n",
    "    )\n",
    "\n",
    "    blocking_pbar(tasmax_futures)\n",
    "except Exception:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "13f1ab6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4cf9242d814a20beb1e3a2d935bcec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    tasmin_futures = client.map(\n",
    "        copy_and_validate,\n",
    "        tasmin_files,\n",
    "        output_version=OUTPUT_VERSION,\n",
    "        check=False,\n",
    "        deep_copy_check=True,\n",
    "        quick_check_and_retry=True,\n",
    "        overwrite=False,\n",
    "        overwrite_on_failure=False,\n",
    "        check_dtr=False,\n",
    "        pbar=False\n",
    "    )\n",
    "\n",
    "    blocking_pbar(tasmin_futures)\n",
    "except Exception:\n",
    "    client.restart()\n",
    "    cluster.scale(0)\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fd6822",
   "metadata": {},
   "source": [
    "### Check tasmax data in final location\n",
    "Check all tasmax values, including bounds & NAN checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "69d97fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89257683fcc54ebfa90607e3adcbeefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    for f in tqdm(tasmax_files):\n",
    "        copy_and_validate(\n",
    "            f,\n",
    "            output_version=OUTPUT_VERSION,\n",
    "            check=True,\n",
    "            deep_copy_check=False,\n",
    "            quick_check_and_retry=False,\n",
    "            overwrite=False,\n",
    "            overwrite_on_failure=True,\n",
    "            check_dtr=False,\n",
    "            pbar=False,\n",
    "        )\n",
    "except Exception:\n",
    "    client.restart()\n",
    "    cluster.scale(0)\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd538e23",
   "metadata": {},
   "source": [
    "### Check tasmin data & DTR in final location\n",
    "Check all tasmin values, including bounds & NAN checks, plus check DTR implied by tasmin & tasmax for positivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "413d662d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6453a42bfbb746eaa22cb6aa51fae4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    for f in tqdm(tasmin_files):\n",
    "        copy_and_validate(\n",
    "            f,\n",
    "            output_version=OUTPUT_VERSION,\n",
    "            check=True,\n",
    "            deep_copy_check=False,\n",
    "            quick_check_and_retry=False,\n",
    "            overwrite=False,\n",
    "            overwrite_on_failure=True,\n",
    "            check_dtr=True,\n",
    "            pbar=False,\n",
    "        )\n",
    "except Exception:\n",
    "    client.restart()\n",
    "    cluster.scale(0)\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "90695e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()\n",
    "cluster.scale(0)\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "833593e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs are located in the following directory: gs://downscaled-288ec5ac/outputs/ScenarioMIP/MIROC/MIROC6/ssp585/r1i1p1f1/day\n"
     ]
    }
   ],
   "source": [
    "outfiles = []\n",
    "for f in (tasmin_files + tasmax_files):\n",
    "    outfiles.append(get_spec_from_input_fp(f)['output_fp'])\n",
    "\n",
    "print(f'outputs are located in the following directory: {os.path.commonpath(outfiles).replace(\"gs:/\", \"gs://\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573baae2",
   "metadata": {},
   "source": [
    "To transfer data elsewhere, such as to prep for public delivery or delivery to Catalyst buckets, contact Mike for help with google transfer utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6173c77e-da5a-4015-97bc-a5fd3b2638d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
