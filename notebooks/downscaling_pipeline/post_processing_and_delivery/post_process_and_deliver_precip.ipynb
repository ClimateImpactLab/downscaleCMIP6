{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151b3d1c-fbb2-4764-ae15-ab80f4cca63c",
   "metadata": {},
   "source": [
    "# Rechunk, cap, Q/A, and deliver precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "74ff668b-94c6-443e-a334-5abd5519bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY = '''\n",
    "v1.1 : initial release (version number set to match temperature). \n",
    "'''.strip()\n",
    "\n",
    "OUTPUT_VERSION = 'v1.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "024290a8-ef86-42a0-8645-57531ea2229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import fsspec\n",
    "import requests\n",
    "import contextlib\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import zarr\n",
    "import rechunker\n",
    "import dask\n",
    "import rhg_compute_tools.kubernetes as rhgk\n",
    "import rhg_compute_tools.utils as rhgu\n",
    "import dask.distributed as dd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from yaml import load, Loader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ed17d4e5-9951-495a-9da5-7072691c1196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/repositories/downscaleCMIP6/notebooks/downscaling_pipeline\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a5a02fc8-7a17-4344-aa55-1ac31bdf224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_prep as up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b4295ba4-7ae6-4f2f-8da5-48f4e0b88a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIVERY_MODELS = up.delivery_models()\n",
    "INSTITUTIONS = up.institutions()\n",
    "ENSEMBLE_MEMBERS = up.ensemble_members_pr()\n",
    "GRID_SPECS = up.grid_specs_pr()\n",
    "HIST_EXTENSION_SCENARIO = up.hist_scn()\n",
    "\n",
    "DATETIME_VERSION = up.datetime_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6009ee24-ee82-4727-9402-efae0be145e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_REF_0p25deg_FP = 'gs://support-c23ff1a3/qplad-fine-reference/pr/v20220201000555.zarr'\n",
    "\n",
    "cleaned_gcm_pattern = (\n",
    "    'gs://clean-b1dbca25/cmip6/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{source_version}.zarr'\n",
    ")\n",
    "\n",
    "downscaled_filepatt = (\n",
    "    'gs://downscaled-288ec5ac/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}.zarr'\n",
    ")\n",
    "\n",
    "rechunked_temp_store_pattern = (\n",
    "    'gs://scratch-170cd6ec/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}-rechunked-temp-store.zarr'\n",
    ")\n",
    "\n",
    "rechunked_pattern = (\n",
    "    'gs://scratch-170cd6ec/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}-rechunked.zarr'\n",
    ")\n",
    "\n",
    "capped_pattern = (\n",
    "    'gs://scratch-170cd6ec/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}-pr-capped.zarr'\n",
    ")\n",
    "\n",
    "OUTPUT_PATTERN = (\n",
    "    'gs://downscaled-288ec5ac/outputs/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{delivery_version}.zarr'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "80480c37-686e-469e-ad8c-c80c9fda8429",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('gs', timeout=120, cache_timeout=120, requests_timeout=120, read_timeout=120, conn_timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "823eec05-0206-44b8-93e4-f8f49e9a32ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd437e09f3548b59e8d626e1b61f3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pr:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(f'data_paths.yaml') as infile:\n",
    "    allpaths = load(infile, Loader)\n",
    "\n",
    "pr_fps = {m: {} for m in DELIVERY_MODELS}\n",
    "\n",
    "for m in tqdm(DELIVERY_MODELS, desc='pr'):\n",
    "\n",
    "    inst = INSTITUTIONS[m]\n",
    "\n",
    "    for act, scen in [\n",
    "        ('CMIP', 'historical'),\n",
    "        ('ScenarioMIP', 'ssp126'),\n",
    "        ('ScenarioMIP', 'ssp245'),\n",
    "        ('ScenarioMIP', 'ssp370'),\n",
    "        ('ScenarioMIP', 'ssp585'),\n",
    "    ]:\n",
    "        if m == 'MPI-ESM1-2-HR' and scen == 'historical':\n",
    "            inst = 'MPI-M'\n",
    "        if scen in allpaths[f'{m}-pr']:\n",
    "            pr_fps[m][scen] = allpaths[f'{m}-pr'][scen]['downscaled']\n",
    "        else:\n",
    "            warnings.warn(f'skipping {m}-{scen} pr as I did not find it in the all paths yaml file')\n",
    "\n",
    "INPUT_FILE_VERSIONS = {\n",
    "    'version': OUTPUT_VERSION,\n",
    "    'created': pd.Timestamp.now(tz='US/Pacific').strftime('%c'),\n",
    "    'history': HISTORY,\n",
    "    'file_paths': {\n",
    "        'pr': pr_fps,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "36a0b37e-8972-4b79-a4f9-ec915ad39b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "CC0_LICENSE_MODELS, CC_BY_LICENSE_MODELS, CC_BY_SA_LICENSE_MODELS = up.licenses()\n",
    "up.check_licenses(delivery_models=DELIVERY_MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "396955bd-c222-44d3-a42e-e5a071cd464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in DELIVERY_MODELS:\n",
    "    hist_extension = HIST_EXTENSION_SCENARIO[m]\n",
    "    if len(INPUT_FILE_VERSIONS['file_paths']['pr'][m]) == 0:\n",
    "        continue\n",
    "\n",
    "    assert hist_extension in INPUT_FILE_VERSIONS['file_paths']['pr'][m].keys(), (\n",
    "        f\"{hist_extension} not in {INPUT_FILE_VERSIONS['file_paths']['pr'][m]} for model {m}\"\n",
    "    )\n",
    "\n",
    "    if hist_extension != 'ssp370':\n",
    "        assert 'ssp370' not in INPUT_FILE_VERSIONS['file_paths']['pr'][m].keys()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a1947fba-f187-433f-9b56-dfa059c64fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_files = [fp for m, v in INPUT_FILE_VERSIONS['file_paths']['pr'].items() for s, fp in v.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d7b4dd-9494-4c93-859a-dfe3b99380d1",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029cafa-e4d0-49e8-957f-329a06ba1619",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "87996b88-1934-4d80-931d-f486b3f44a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=[\n",
    "    'downscaled_filepatt',\n",
    "    'rechunked_temp_store_pattern',\n",
    "    'rechunked_pattern',\n",
    "    'capped_pattern',\n",
    "    'OUTPUT_PATTERN',\n",
    "])\n",
    "def get_spec_from_input_fp(fp, output_version=OUTPUT_VERSION, datetime_version=DATETIME_VERSION):\n",
    "    (\n",
    "        bucket,\n",
    "        stage,\n",
    "        activity,\n",
    "        institution,\n",
    "        model,\n",
    "        scenario,\n",
    "        ensemble,\n",
    "        table,\n",
    "        variable,\n",
    "        grid,\n",
    "        run_version,\n",
    "    ) = os.path.splitext(fp)[0].replace('gs://', '').split('/')\n",
    "\n",
    "    spec = dict(\n",
    "        bucket=bucket,\n",
    "        stage=stage,\n",
    "        activity=activity,\n",
    "        institution=institution,\n",
    "        model=model,\n",
    "        scenario=scenario,\n",
    "        ensemble=ensemble,\n",
    "        table=table,\n",
    "        variable=variable,\n",
    "        grid=grid,\n",
    "        run_version=run_version,\n",
    "    )\n",
    "\n",
    "    for (name, fpatt) in [\n",
    "        ('downscaled_fp', downscaled_filepatt),\n",
    "        ('output_fp', OUTPUT_PATTERN),\n",
    "    ]:\n",
    "        spec[name] = fpatt.format(\n",
    "            activity_id=activity,\n",
    "            institution_id=institution,\n",
    "            source_id=model,\n",
    "            experiment_id=scenario,\n",
    "            member_id=ensemble,\n",
    "            variable_id=variable,\n",
    "            table_id=table,\n",
    "            grid_spec=grid,\n",
    "            run_version = run_version,\n",
    "            delivery_version=output_version,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    for (name, fpatt) in [\n",
    "        ('rechunk_temp_store_fp', rechunked_temp_store_pattern),\n",
    "        ('rechunked_fp', rechunked_pattern),\n",
    "        ('capped_fp', capped_pattern),\n",
    "    ]:\n",
    "        spec[name] = fpatt.format(\n",
    "            activity_id=activity,\n",
    "            institution_id=institution,\n",
    "            source_id=model,\n",
    "            experiment_id=scenario,\n",
    "            member_id=ensemble,\n",
    "            variable_id=variable,\n",
    "            table_id=table,\n",
    "            grid_spec=grid,\n",
    "            run_version = datetime_version,\n",
    "            delivery_version=output_version,\n",
    "        )\n",
    "\n",
    "    return spec\n",
    "\n",
    "@rhgu.block_globals\n",
    "def get_spec_from_output_fp(fp, output_pattern=OUTPUT_PATTERN):\n",
    "    \n",
    "    (\n",
    "        bucket,\n",
    "        stage,\n",
    "        activity,\n",
    "        institution,\n",
    "        model,\n",
    "        scenario,\n",
    "        ensemble,\n",
    "        table,\n",
    "        variable,\n",
    "        output_version,\n",
    "    ) = os.path.splitext(fp)[0].replace('gs://', '').split('/')\n",
    "\n",
    "    output_fp = output_pattern.format(\n",
    "        activity_id=activity,\n",
    "        institution_id=institution,\n",
    "        source_id=model,\n",
    "        experiment_id=scenario,\n",
    "        member_id=ensemble,\n",
    "        variable_id=variable,\n",
    "        table_id=table,\n",
    "        delivery_version=output_version,\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "        activity=activity,\n",
    "        institution=institution,\n",
    "        model=model,\n",
    "        scenario=scenario,\n",
    "        ensemble=ensemble,\n",
    "        table=table,\n",
    "        variable=variable,\n",
    "        output_version=output_version,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb9d367-2108-49ba-8957-7be42927f6c1",
   "metadata": {},
   "source": [
    "## Stage 1: Rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4b280a82-83d3-4cd9-bea9-3ed1598447f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=[\n",
    "    'INPUT_FILE_VERSIONS',\n",
    "])\n",
    "def rechunk_data(varname, model, scenario, worker_memory_limit):\n",
    "\n",
    "    fs = fsspec.filesystem('gs', timeout=120, cache_timeout=120, requests_timeout=120, read_timeout=120, conn_timeout=120)\n",
    "\n",
    "    target_chunks = {\n",
    "        varname: {'time': 365, 'lat': 360, 'lon': 360},\n",
    "        'time': {'time': 365},\n",
    "        'lat': {'lat': 360},\n",
    "        'lon': {'lon': 360},\n",
    "    }\n",
    "\n",
    "    input_fp = INPUT_FILE_VERSIONS['file_paths'][varname][model][scenario]\n",
    "    input_spec = get_spec_from_input_fp(input_fp)\n",
    "\n",
    "    rechunked_temp_store_fp = input_spec['rechunk_temp_store_fp']\n",
    "    rechunked_fp = input_spec['rechunked_fp']\n",
    "\n",
    "    mapper = fs.get_mapper(input_fp)\n",
    "    with xr.open_zarr(mapper) as ds:\n",
    "\n",
    "        rechunked_mapper = fs.get_mapper(rechunked_fp)\n",
    "\n",
    "        chunk_job = rechunker.rechunk(\n",
    "            source=ds,\n",
    "            target_chunks=target_chunks,\n",
    "            max_mem=worker_memory_limit,\n",
    "            target_store=rechunked_mapper,\n",
    "            temp_store=fs.get_mapper(rechunked_temp_store_fp),\n",
    "        )\n",
    "\n",
    "        chunk_job_persist = chunk_job._plan.persist()\n",
    "        dd.wait(chunk_job_persist)\n",
    "\n",
    "    zarr.convenience.consolidate_metadata(rechunked_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c326ce-bb80-463e-a7e7-302571abddd4",
   "metadata": {},
   "source": [
    "## Stage 2: Cap precip at the max(max)*max(max)/max(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "24f74793-15f2-4e28-81f7-5a012ae2cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=['cleaned_gcm_pattern', 'HIST_EXTENSION_SCENARIO', 'INPUT_FILE_VERSIONS', 'CLEANED_REF_0p25deg_FP'])\n",
    "def cap_precip(pr_input_fp):\n",
    "\n",
    "    fs = fsspec.filesystem('gs')\n",
    "\n",
    "    pr_spec = get_spec_from_input_fp(pr_input_fp)\n",
    "\n",
    "    dest_fp = pr_spec['capped_fp']\n",
    "\n",
    "    gcm_rechunked = xr.open_zarr(fs.get_mapper(pr_spec['rechunked_fp']))\n",
    "    \n",
    "    source_file_versions = INPUT_FILE_VERSIONS['file_paths']['pr'][pr_spec['model']]\n",
    "\n",
    "    if pr_spec['scenario'] == 'historical':\n",
    "        proj_scen = HIST_EXTENSION_SCENARIO[pr_spec['model']]\n",
    "        proj_fp = get_spec_from_input_fp(source_file_versions[proj_scen])['rechunked_fp']\n",
    "        print(proj_fp)\n",
    "        with xr.open_zarr(fs.get_mapper(proj_fp)) as proj:\n",
    "            source_version_proj = proj.attrs['version_id']\n",
    "\n",
    "        source_version_hist = gcm_rechunked.attrs['version_id']\n",
    "        \n",
    "    else:\n",
    "        proj_scen = pr_spec['scenario']\n",
    "        hist_fp = get_spec_from_input_fp(source_file_versions['historical'])['rechunked_fp']\n",
    "        print(hist_fp)\n",
    "        with xr.open_zarr(fs.get_mapper(hist_fp)) as hist:\n",
    "            source_version_hist = hist.attrs['version_id']\n",
    "\n",
    "        source_version_proj = gcm_rechunked.attrs['version_id']\n",
    "\n",
    "    source_version_id = gcm_rechunked.attrs['version_id']\n",
    "\n",
    "    if pr_spec['model'] == 'MPI-ESM1-2-HR':\n",
    "        insthist = 'MPI-M'\n",
    "    else:\n",
    "        insthist = pr_spec['institution']\n",
    "    \n",
    "    clean_fp_hist = cleaned_gcm_pattern.format(\n",
    "        activity_id='CMIP',\n",
    "        institution_id=insthist,\n",
    "        source_id=pr_spec['model'],\n",
    "        experiment_id='historical',\n",
    "        member_id=pr_spec['ensemble'],\n",
    "        table_id=pr_spec['table'],\n",
    "        variable_id=pr_spec['variable'],\n",
    "        grid_spec=pr_spec['grid'],\n",
    "        source_version=source_version_hist,\n",
    "    )\n",
    "\n",
    "    if pr_spec['model'] == 'MPI-ESM1-2-HR':\n",
    "        instproj = 'DKRZ'\n",
    "    else:\n",
    "        instproj = pr_spec['institution']\n",
    "        \n",
    "    clean_fp_proj = cleaned_gcm_pattern.format(\n",
    "        activity_id='ScenarioMIP',\n",
    "        institution_id=instproj,\n",
    "        source_id=pr_spec['model'],\n",
    "        experiment_id=proj_scen,\n",
    "        member_id=pr_spec['ensemble'],\n",
    "        table_id=pr_spec['table'],\n",
    "        variable_id=pr_spec['variable'],\n",
    "        grid_spec=pr_spec['grid'],\n",
    "        source_version=source_version_proj,\n",
    "    )\n",
    "\n",
    "    ref_fp = CLEANED_REF_0p25deg_FP\n",
    "    \n",
    "    try:\n",
    "        clean_hist = xr.open_zarr(fs.get_mapper(clean_fp_hist))\n",
    "    except zarr.errors.GroupNotFoundError:\n",
    "        raise FileNotFoundError(clean_fp_hist)\n",
    "\n",
    "    try:\n",
    "        clean_proj = xr.open_zarr(fs.get_mapper(clean_fp_proj))\n",
    "    except zarr.errors.GroupNotFoundError:\n",
    "        raise FileNotFoundError(clean_fp_proj)\n",
    "\n",
    "    ref = xr.open_zarr(fs.get_mapper(ref_fp))\n",
    "\n",
    "    ref_maxpr = ref.sel(time=slice('1994-12-16', '2015-01-15')).pr.max(dim='time').compute()\n",
    "\n",
    "    gcm_hist_maxpr = clean_hist.sel(time=slice('1994-12-16', '2015-01-15')).pr.max(dim='time').compute()\n",
    "\n",
    "    gcm_proj_maxpr = (\n",
    "        xr.concat([clean_hist, clean_proj], dim='time')\n",
    "        .pr\n",
    "        .groupby('time.year')\n",
    "        .max(dim='time')\n",
    "        .compute()\n",
    "    )\n",
    "\n",
    "    # convert lons to [-180, 180]\n",
    "\n",
    "    gcm_hist_maxpr = (\n",
    "        gcm_hist_maxpr\n",
    "        .assign_coords(lon=((gcm_hist_maxpr.lon % 360 + 180) % 360 - 180))\n",
    "        .sortby('lon')\n",
    "    )\n",
    "\n",
    "    gcm_proj_maxpr = (\n",
    "        gcm_proj_maxpr\n",
    "        .assign_coords(lon=((gcm_proj_maxpr.lon % 360 + 180) % 360 - 180))\n",
    "        .sortby('lon')\n",
    "    )\n",
    "    \n",
    "    gcm_proj_maxpr_rolled = (\n",
    "        gcm_proj_maxpr\n",
    "        .rolling(year=21, center=True, min_periods=21).max()\n",
    "    )\n",
    "\n",
    "    gcm_factor = (\n",
    "        (gcm_proj_maxpr_rolled.dropna(dim='year', how='all') / gcm_hist_maxpr)\n",
    "        .rename({'lat': 'lat_coarse', 'lon': 'lon_coarse'})\n",
    "        .sel(lat_coarse=ref_maxpr.lat, lon_coarse=ref_maxpr.lon, method='nearest')\n",
    "        .drop(['lat_coarse', 'lon_coarse'])\n",
    "    )\n",
    "\n",
    "    upper_bound = np.maximum(1, gcm_factor) * ref_maxpr\n",
    "    \n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        upper_bound_full = (\n",
    "            upper_bound\n",
    "            .reindex(year=np.unique(gcm_rechunked.time.dt.year), method='nearest')\n",
    "            .chunk({'year': 1})\n",
    "            .sel(year=gcm_rechunked.time.dt.year)\n",
    "            .drop('year')\n",
    "        )\n",
    "\n",
    "        gcm_capped = gcm_rechunked.copy(deep=False)\n",
    "\n",
    "        gcm_capped['pr'] = np.minimum(upper_bound_full, gcm_rechunked['pr'])\n",
    "        gcm_capped['pr'].attrs = gcm_rechunked['pr'].attrs\n",
    "        gcm_capped.attrs = gcm_rechunked.attrs\n",
    "\n",
    "        out_mapper = fs.get_mapper(dest_fp)\n",
    "        gcm_capped.to_zarr(out_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331144c-65e9-49fb-b339-34329b371229",
   "metadata": {},
   "source": [
    "## Stage 3: copy to destination directory & validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f7820315-4898-48e9-a5ef-19b8c049bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=['CC0_LICENSE_MODELS', 'CC_BY_LICENSE_MODELS'])\n",
    "def quick_check_file(fp, ds, spec, CC0_LICENSE_MODELS=CC0_LICENSE_MODELS, CC_BY_LICENSE_MODELS=CC_BY_LICENSE_MODELS, CC_BY_SA_LICENSE_MODELS=CC_BY_SA_LICENSE_MODELS):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # check that metadata matches file spec\n",
    "\n",
    "    assert ds.attrs['institution_id'] == spec['institution'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['institution_id']} ≠ {spec['institution']}\"\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['source_id'] == spec['model'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['source_id']} ≠ {spec['model']}\"\n",
    "    )\n",
    "\n",
    "    assert spec['activity'] in ds.attrs['activity_id'], (\n",
    "        f\"invalid attrs in {fp}: {spec['activity']} not in {ds.attrs['activity_id']}\"\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['experiment_id'] == spec['scenario'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['experiment_id']} ≠ {spec['scenario']}\"\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['variant_label'] == spec['ensemble'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['variant_label']} ≠ {spec['ensemble']}\"\n",
    "    )\n",
    "\n",
    "    if spec['variable'] == 'tasmax':\n",
    "        assert ds['tasmax'].attrs['long_name'] == 'Daily Maximum Near-Surface Air Temperature'\n",
    "        assert ds['tasmax'].attrs['units'] == 'K'\n",
    "    elif spec['variable'] == 'tasmin':\n",
    "        assert ds['tasmin'].attrs['long_name'] == 'Daily Minimum Near-Surface Air Temperature'\n",
    "        assert ds['tasmin'].attrs['units'] == 'K'\n",
    "    elif spec['variable'] == 'pr':\n",
    "#         assert ds['pr'].attrs['long_name'] == \n",
    "        assert ds['pr'].attrs['units'] == 'mm day-1'\n",
    "    else:\n",
    "        raise ValueError(f'variable not recognized: {spec[\"variable\"]}')\n",
    "\n",
    "    # Check licensing fields & endpoint URL\n",
    "\n",
    "    # check that license URL points to a real location and it exists\n",
    "    license_url = ds.attrs['license']\n",
    "    assert ds.attrs['source_id'] in license_url, (\n",
    "        f'model \"{ds.attrs[\"source_id\"]}\" not found in license url: {license_url}'\n",
    "    )\n",
    "    r = requests.get(license_url)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # check that \"Creaive Commons\" and the model name show up on the page\n",
    "    assert ds.attrs['source_id'] in r.text, (\n",
    "        f'model \"{ds.attrs[\"source_id\"]}\" not found on license page: {license_url}'\n",
    "    )\n",
    "\n",
    "    assert \"Creative Commons\" in r.text, (\n",
    "        f'\"Creative Commons\" not found on license page: {license_url}'\n",
    "    )\n",
    "\n",
    "    # check that \"Creative Commons\" appears in the raw license text\n",
    "\n",
    "    raw_license_url = (\n",
    "        ds.attrs['license']\n",
    "        .replace('github.com', 'raw.githubusercontent.com')\n",
    "        .replace('/blob/', '/')\n",
    "        .replace('/tree/', '/')\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['source_id'] in raw_license_url, (\n",
    "        f'model \"{ds.attrs[\"source_id\"]}\" not found in license url: {raw_license_url}'\n",
    "    )\n",
    "    r = requests.get(raw_license_url)\n",
    "    r.raise_for_status()\n",
    "    assert 'Creative Commons' in r.text, (\n",
    "        f'\"Creative Commons\" not found in license text: {raw_license_url}'\n",
    "    )\n",
    "\n",
    "    if spec['model'] in CC0_LICENSE_MODELS:\n",
    "        assert 'CC0 1.0 Universal' in r.text, (\n",
    "            f\"expected CC0 license for {spec['model']} at {fp}\"\n",
    "        )\n",
    "    elif spec['model'] in CC_BY_LICENSE_MODELS:\n",
    "        assert 'Attribution 4.0 International' in r.text, (\n",
    "            f\"expected CC-BY 4.0 license for {spec['model']} at {fp}\"\n",
    "        )\n",
    "    elif spec['model'] in CC_BY_SA_LICENSE_MODELS:\n",
    "        assert 'Attribution-ShareAlike 4.0 International' in r.text, (\n",
    "            f\"expected CC-BY-SA 4.0 license for {spec['model']} at {fp}\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"deploying model with unknown license: {spec['model']} at {fp}\"\n",
    "        )\n",
    "\n",
    "    # Check dimension size & membership\n",
    "\n",
    "    for c in ds.coords.keys():\n",
    "        assert ds.coords[c].notnull().all().item() is True, f\"NaNs found in coordinate '{c}' in {fp}\"\n",
    "\n",
    "    if spec['activity'] == 'ScenarioMIP':\n",
    "        date_range = xr.cftime_range(\"2015-01-01\", \"2099-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "        if len(ds.time) > len(date_range):\n",
    "            date_range = xr.cftime_range(\"2015-01-01\", \"2100-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "    else:\n",
    "        date_range = xr.cftime_range(\"1950-01-01\", \"2014-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "\n",
    "    assert ds.sizes['time'] == len(date_range), (\n",
    "        f\"unexpected length of dimension 'time': length {len(ds.time)}; \"\n",
    "        f\"expected {len(date_range)} in {fp}\"\n",
    "    )\n",
    "\n",
    "    assert date_range.isin(ds.time.dt.floor('D').values).all(), f\"invalid coords in {fp}\"\n",
    "\n",
    "    assert pd.Series(np.arange(-179.875, 180, 0.25)).isin(ds.lon.values).all(), (\n",
    "        f\"invalid coords in {fp}\"\n",
    "    )\n",
    "    assert pd.Series(np.arange(-89.875, 90, 0.25)).isin(ds.lat.values).all(), (\n",
    "        f\"invalid coords in {fp}\"\n",
    "    )\n",
    "\n",
    "    varnames = list(ds.data_vars.keys())\n",
    "    assert len(varnames) == 1\n",
    "    varname = varnames[0]\n",
    "\n",
    "    assert ds[varname].sizes['lat'] == 720, f\"lat not length 720 in {fp}:\\n{ds}\"\n",
    "    assert ds[varname].sizes['lon'] == 1440, f\"lon not length 1440 in {fp}:\\n{ds}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "24ec495f-c345-47eb-beb4-f37821e754f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=['INPUT_FILE_VERSIONS'])\n",
    "def validate_outputs(fp, quick=False):\n",
    "    spec = get_spec_from_output_fp(fp)\n",
    "\n",
    "    fs = fsspec.filesystem('gs', timeout=60, cache_timeout=60, requests_timeout=60, read_timeout=60, conn_timeout=60)\n",
    "\n",
    "    mapper = fs.get_mapper(fp)\n",
    "\n",
    "    with xr.open_zarr(mapper) as ds:\n",
    "\n",
    "        quick_check_file(fp, ds, spec)\n",
    "\n",
    "        if quick:\n",
    "            return\n",
    "\n",
    "        # check variable contents\n",
    "\n",
    "        varnames = list(ds.data_vars.keys())\n",
    "        assert len(varnames) == 1\n",
    "        varname = varnames[0]\n",
    "\n",
    "        to_check = ds[varname].sel(lat=slice(-80, 80))\n",
    "\n",
    "        nans = to_check.isnull().any()\n",
    "        min_val = to_check.min()\n",
    "        max_val = to_check.max()\n",
    "\n",
    "        nans, vmin, vmax = dd.get_client().compute(\n",
    "            [nans, min_val, max_val],\n",
    "            optimize_graph=True,\n",
    "            sync=True,\n",
    "            retries=3,\n",
    "        )\n",
    "\n",
    "        assert nans.item() is False, f\"NaNs found in {fp}\"\n",
    "\n",
    "        if varname == 'tasmax':\n",
    "            allowed_min = 150\n",
    "            allowed_max = 360\n",
    "        elif varname == 'tasmin':\n",
    "            allowed_min = 150\n",
    "            allowed_max = 360\n",
    "        elif varname == 'pr':\n",
    "            allowed_min = 0\n",
    "            allowed_max = 3000\n",
    "        else:\n",
    "            raise ValueError(f'Variable name not recognized: {varname}\\nin file: {fp}')\n",
    "\n",
    "        assert (vmin >= allowed_min).item() is True, (\n",
    "            f\"min value {vmin} outside allowed range [{allowed_min}, {allowed_max}] \"\n",
    "            f\"for {varname} in {fp}\"\n",
    "        )\n",
    "        assert (vmax <= allowed_max).item() is True, (\n",
    "            f\"max value {vmax} outside allowed range [{allowed_min}, {allowed_max}] \"\n",
    "            f\"for {varname} in {fp}\"\n",
    "        )\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def copy_and_validate(\n",
    "    source_fp,\n",
    "    output_version=OUTPUT_VERSION,\n",
    "    check=False,\n",
    "    deep_copy_check=False,\n",
    "    quick_check_and_retry=True,\n",
    "    overwrite=False,\n",
    "    overwrite_on_failure=False,\n",
    "    pbar=False,\n",
    "):\n",
    "\n",
    "    spec = get_spec_from_input_fp(source_fp, output_version=output_version)\n",
    "    capped_fp = spec['capped_fp']\n",
    "    output_fp = spec['output_fp']\n",
    "    model = spec['model']\n",
    "    scenario = spec['scenario']\n",
    "\n",
    "    fs = fsspec.filesystem(\n",
    "        'gs',\n",
    "        timeout=360,\n",
    "        cache_timeout=360,\n",
    "        requests_timeout=360, read_timeout=360, conn_timeout=360)\n",
    "\n",
    "    if fs.exists(output_fp):\n",
    "        if overwrite:\n",
    "            fs.remove(output_fp, recursive=True)\n",
    "\n",
    "        else:\n",
    "            if deep_copy_check:\n",
    "                dirs = list([(d, f) for d, dirs, fps in fs.walk(capped_fp) for f in fps])\n",
    "                if pbar:\n",
    "                    dirs = tqdm(dirs)\n",
    "\n",
    "                for d, f in dirs:\n",
    "                    src = capped_fp[:5] + os.path.join(d, f)\n",
    "                    dst = os.path.join(output_fp, os.path.relpath(src, capped_fp))\n",
    "                    assert '..' not in dst\n",
    "                    src_hash = fs.stat(src)['md5Hash']\n",
    "\n",
    "                    for i in range(5):\n",
    "                        try:\n",
    "                            assert (src_hash == fs.stat(dst)['md5Hash'])\n",
    "                            break\n",
    "                        except (FileNotFoundError, AssertionError):\n",
    "                            if i == 4:\n",
    "                                raise\n",
    "\n",
    "                            fs.rm(dst)\n",
    "                            fs.copy(src, dst)\n",
    "\n",
    "            if check:\n",
    "                try:\n",
    "                    validate_outputs(\n",
    "                        output_fp,\n",
    "                    )\n",
    "                    return\n",
    "                except (\n",
    "                    AssertionError,\n",
    "                    FileNotFoundError,\n",
    "                    ValueError,\n",
    "                    IOError,\n",
    "                    xr.coding.times.OutOfBoundsDatetime,\n",
    "                    OverflowError,\n",
    "                ):\n",
    "                    if overwrite_on_failure:\n",
    "                        import warnings\n",
    "                        warnings.warn(f'validation failed for {output_fp}, deleting file')\n",
    "                        fs.rm(output_fp, recursive=True)\n",
    "                        return\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            elif quick_check_and_retry:\n",
    "                try:\n",
    "                    validate_outputs(output_fp, quick=True)\n",
    "                    return\n",
    "                except (\n",
    "                    OverflowError,\n",
    "                    IOError,\n",
    "                    zarr.errors.GroupNotFoundError,\n",
    "                    FileNotFoundError,\n",
    "                    AssertionError,\n",
    "                    ValueError,\n",
    "                ):\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                return\n",
    "\n",
    "    print(f'copying:\\n\\tsrc:\\t{capped_fp}\\n\\tdst:\\t{output_fp}')\n",
    "    fs.copy(capped_fp, output_fp, recursive=True, batch_size=1000)\n",
    "\n",
    "    if deep_copy_check:\n",
    "        for d, f in list([(d, f) for d, dirs, fps in fs.walk(capped_fp) for f in fps]):\n",
    "            src = capped_fp[:5] + os.path.join(d, f)\n",
    "            dst = os.path.join(output_fp, os.path.relpath(src, capped_fp))\n",
    "            assert '..' not in dst\n",
    "            src_hash = fs.stat(src)['md5Hash']\n",
    "\n",
    "            for i in range(5):\n",
    "                try:\n",
    "                    assert (src_hash == fs.stat(dst)['md5Hash'])\n",
    "                    break\n",
    "                except (FileNotFoundError, AssertionError):\n",
    "                    if i == 4:\n",
    "                        raise\n",
    "\n",
    "                    fs.rm(dst)\n",
    "                    fs.copy(src, dst)\n",
    "\n",
    "    if check:\n",
    "        validate_outputs(\n",
    "            output_fp,\n",
    "        )\n",
    "    elif quick_check_and_retry:\n",
    "        validate_outputs(output_fp, quick=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6f4bb856-3877-4bc3-a99d-2e629bb21011",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def kill_cluster_on_error():\n",
    "    try:\n",
    "        yield\n",
    "    except Exception:\n",
    "        # kill the cluster if something unexpected happens during a long-running job\n",
    "        client.restart()\n",
    "        cluster.scale(0)\n",
    "        client.close()\n",
    "        cluster.close()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a21cb8b0-2f9e-4fee-a9f5-1b569f690ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blocking_pbar(futures):\n",
    "    status = {'error': 0, 'killed': 0, 'lost': 0}\n",
    "    with tqdm(dd.as_completed(futures), total=len(futures)) as pbar:\n",
    "        for f in pbar:\n",
    "            if f.status in status.keys():\n",
    "                status[f.status] += 1\n",
    "                pbar.set_postfix(status)\n",
    "\n",
    "    dd.wait(futures)\n",
    "    for bad_status in status.keys():\n",
    "        if status[bad_status] > 0:\n",
    "            [f for f in futures if f.status == bad_status][0].result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3900e5-bcc7-44c4-ad61-8a840c4ed4f1",
   "metadata": {},
   "source": [
    "# Full workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9d85359d-5423-4904-a0e0-df79eb486d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://jhub-dev.onyx.climateriskservice.com/services/dask-gateway/clusters/jhub.9b31ab40678c4f15a6a6839c5435b8a2/status\n"
     ]
    }
   ],
   "source": [
    "client, cluster = rhgk.get_giant_cluster()\n",
    "N_WORKERS = 60\n",
    "cluster.scale(N_WORKERS)\n",
    "MAX_MEM = '12GB' # for standard cluster\n",
    "cluster\n",
    "# wait until the workers come online - otherwise we get unbalanced task loading and things start acting all fishy\n",
    "while len(client.ncores()) < N_WORKERS:\n",
    "    time.sleep(5)\n",
    "print('https://jhub-dev.onyx.climateriskservice.com' + cluster.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0fe81a-cada-4f13-afc6-051c8a6cf9bc",
   "metadata": {},
   "source": [
    "# Prepare final outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59af1c52-ae9d-4280-8c37-9232885cbd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n",
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/tornado/iostream.py\", line 1391, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/ssl.py\", line 1309, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1131)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 189, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/tornado/iostream.py\", line 696, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/tornado/iostream.py\", line 1478, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/tornado/iostream.py\", line 1400, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/tornado/iostream.py\", line 611, in close\n",
      "    self._signal_closed()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/tornado/iostream.py\", line 641, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "with kill_cluster_on_error():\n",
    "    with tqdm(DELIVERY_MODELS) as pbar:\n",
    "        for model in pbar:\n",
    "            for scenario in INPUT_FILE_VERSIONS['file_paths']['pr'][model].keys():\n",
    "\n",
    "                pr_input_fp = INPUT_FILE_VERSIONS['file_paths']['pr'][model][scenario]\n",
    "                pr_spec = get_spec_from_input_fp(pr_input_fp)\n",
    "\n",
    "#                comment out this block to reproduce rechunked/capped data on scratch bucket\n",
    "#                 if fs.exists(pr_spec['output_fp']):\n",
    "#                     print(f'skipping {model} {scenario} - output already exists')\n",
    "#                     continue\n",
    "\n",
    "                pbar.set_postfix({'model': model, 'scen': scenario, 'stage': 'rechunk pr'})\n",
    "                rechunk_data('pr', model, scenario, worker_memory_limit=MAX_MEM)\n",
    "\n",
    "            for scenario in INPUT_FILE_VERSIONS['file_paths']['pr'][model].keys():\n",
    "\n",
    "                pr_input_fp = INPUT_FILE_VERSIONS['file_paths']['pr'][model][scenario]\n",
    "                pr_spec = get_spec_from_input_fp(pr_input_fp)\n",
    "\n",
    "#                 comment out this block to reproduce rechunked/capped data on scratch bucket\n",
    "#                 if fs.exists(pr_spec['output_fp']):\n",
    "#                     print(f'skipping {model} {scenario} - output already exists')\n",
    "#                     continue\n",
    "\n",
    "                pbar.set_postfix({'model': model, 'scen': scenario, 'stage': 'cap precip'})\n",
    "                with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "                    cap_precip(pr_input_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c9a6c-c8cc-45fe-a7a7-bbd26f75dc77",
   "metadata": {},
   "source": [
    "# Copy files to final destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "680ecb52-a9f3-4d89-a2d4-a08576c66283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9771de922a9e4159914b29793baf4d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with kill_cluster_on_error():\n",
    "    pr_futures = client.map(\n",
    "        copy_and_validate,\n",
    "        pr_files,\n",
    "        output_version=OUTPUT_VERSION,\n",
    "        check=False,\n",
    "        deep_copy_check=False,\n",
    "        quick_check_and_retry=True,\n",
    "        overwrite=False,\n",
    "        overwrite_on_failure=False,\n",
    "        pbar=False,\n",
    "    )\n",
    "\n",
    "    blocking_pbar(pr_futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a17caf-9ee8-44a5-b65e-1475afd7c7f4",
   "metadata": {},
   "source": [
    "# Deep copy check\n",
    "Check every file against source to ensure a complete copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e772bdc-e317-4586-954e-e93a3dad5991",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75bc2bd5db39497ebab69c456f699b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with kill_cluster_on_error():\n",
    "#     pr_futures = client.map(\n",
    "#         copy_and_validate,\n",
    "#         pr_files,\n",
    "#         output_version=OUTPUT_VERSION,\n",
    "#         check=False,\n",
    "#         deep_copy_check=True,\n",
    "#         quick_check_and_retry=True,\n",
    "#         overwrite=False,\n",
    "#         overwrite_on_failure=False,\n",
    "#         pbar=False,\n",
    "#     )\n",
    "\n",
    "#     blocking_pbar(pr_futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c37273-5de3-4bf5-a6ef-46cf6e64defe",
   "metadata": {},
   "source": [
    "### Check pr data in final location\n",
    "Check all pr values, including bounds & NAN checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f9a438ad-4018-41d7-95cb-a7a0065747db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48026b7181664af4af6fe5729436fc4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with kill_cluster_on_error():\n",
    "#     for f in tqdm(pr_files):\n",
    "#         copy_and_validate(\n",
    "#             f,\n",
    "#             output_version=OUTPUT_VERSION,\n",
    "#             check=True,\n",
    "#             deep_copy_check=False,\n",
    "#             quick_check_and_retry=False,\n",
    "#             overwrite=False,\n",
    "#             overwrite_on_failure=True,\n",
    "#             pbar=False,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9bb5e885-0bcf-49fd-9df3-fc94d04dfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()\n",
    "cluster.scale(0)\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8ba0ca30-e20e-41c7-9430-aef09efd7deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs are located in the following directory: gs://downscaled-288ec5ac/outputs\n"
     ]
    }
   ],
   "source": [
    "outfiles = []\n",
    "for f in (pr_files):\n",
    "    outfiles.append(get_spec_from_input_fp(f)['output_fp'])\n",
    "\n",
    "print(f'outputs are located in the following directory: {os.path.commonpath(outfiles).replace(\"gs:/\", \"gs://\").replace(\":///\", \"://\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5651b-8736-4c3e-9d99-62426b7f6e38",
   "metadata": {},
   "source": [
    "To transfer data elsewhere, such as to prep for public delivery or delivery to Catalyst buckets, contact Mike for help with google transfer utility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
