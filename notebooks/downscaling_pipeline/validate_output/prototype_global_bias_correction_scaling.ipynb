{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import xarray as xr\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skdownscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.distributed as dd\n",
    "import rhg_compute_tools.kubernetes as rhgk\n",
    "\n",
    "from utils import _convert_lons, _remove_leap_days, _convert_ds_longitude\n",
    "from regridding import apply_weights\n",
    "\n",
    "import intake\n",
    "import xesmf as xe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output directory\n",
    "write_direc = '/gcs/rhg-data/climate/downscaled/workdir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements a scaling test for bias correction, using the BCSDTemperature model from `scikit-downscale`, with the daily BCSD bias correction method as implemented in the NASA-NEX dataset. \n",
    "\n",
    "Datasets used include a CMIP6 model from a historical run (`GISS-E2-1-G` from NASA) and GMFD (obs). Historical/training period is taken as 1980-1982, and the future/predict period is 1990-1991. \n",
    "\n",
    "GMFD is coarsened to the NASA `GISS-E2-1-G` grid for this bias correction test. \n",
    "\n",
    "Note that the purpose of this notebook is intended to allow us to get a better estimate of timing for global daily bias correction. Future work will build on this notebook to: \n",
    "- replace GMFD with ERA5\n",
    "- combine this notebook with `SD_prototype.ipynb`, along with NASA-NEX data and a corresponding CMIP5 model, and over a limited domain, to test our implementation of BCSD against NASA-NEX for a limited domain. That notebook will be used as a prototype for our downscaling pipeline and can be modified to become a system test for the pipeline (1-3 gridcells for CI/CD, limited domain for science testing). \n",
    "\n",
    "This notebook was also used as a resource and checkpoint for this workflow: https://github.com/jhamman/scikit-downscale/blob/ecahm2020/examples/2020ECAHM-scikit-downscale.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client, cluster = rhgk.get_standard_cluster(extra_pip_packages=\"git+https://github.com/dgergel/xsd.git@feature/implement_daily_bcsd\")\n",
    "client, cluster = rhgk.get_standard_cluster(extra_pip_packages=\"git+https://github.com/jhamman/scikit-downscale.git\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''a = da.ones((1000, 1000, 1000))\n",
    "a.mean().compute()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skdownscale.pointwise_models import PointWiseDownscaler, BcsdTemperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slice = slice('1980', '1989')  # train time range\n",
    "holdout_slice = slice('1990', '2000')  # prediction time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.get_versions(check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GMFD as standin for ERA-5\n",
    "tmax_obs = xr.open_mfdataset(os.path.join('/gcs/rhg-data/climate/source_data/GMFD/tmax', \n",
    "                                         'tmax_0p25_daily_198*'), concat_dim='time', combine='nested',\n",
    "                              parallel=True).squeeze(drop=True).rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "\n",
    "'''tmax_obs = xr.open_dataset(os.path.join('/gcs/rhg-data/climate/source_data/GMFD/tmax', \n",
    "                                        'tmax_0p25_daily_1980-1980.nc')).rename({'latitude': 'lat', 'longitude': 'lon'})'''\n",
    "\n",
    "# standardize longitudes \n",
    "tmax_obs =  _convert_ds_longitude(tmax_obs, lon_name='lon')\n",
    "\n",
    "# remove leap days \n",
    "tmax_obs = _remove_leap_days(tmax_obs)\n",
    "\n",
    "obs_subset = tmax_obs.sel(time=train_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get some CMIP6 data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search the cmip6 catalog\n",
    "col = intake.open_esm_datastore(\"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\")\n",
    "cat = col.search(experiment_id=['historical', 'ssp585'], table_id='day', variable_id='tasmax',\n",
    "                 grid_label='gn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat['CMIP.NASA-GISS.GISS-E2-1-G.historical.day.gn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the data and do some cleanup\n",
    "ds_model = cat['CMIP.NASA-GISS.GISS-E2-1-G.historical.day.gn'].to_dask(\n",
    "           ).isel(member_id=0).squeeze(drop=True).drop(['height', 'lat_bnds', 'lon_bnds', 'time_bnds', \n",
    "                                                        'member_id'])\n",
    "\n",
    "ds_model.lon.values[ds_model.lon.values > 180] -= 360\n",
    "ds_model = ds_model.roll(lon=72, roll_coords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regrid obs to model resolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first rechunk in space for xESMF \n",
    "chunks = {'lat': len(obs_subset.lat), 'lon': len(obs_subset.lon), 'time': 100}\n",
    "obs_subset = obs_subset.chunk(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "obs_to_mod_weights = os.path.join(write_direc, 'bias_correction_bilinear_weights_new.nc')\n",
    "\n",
    "regridder_obs_to_mod = xe.Regridder(obs_subset.isel(time=0, drop=True), \n",
    "                                    ds_model.isel(time=0, drop=True), \n",
    "                                    'bilinear', \n",
    "                                    filename=obs_to_mod_weights, \n",
    "                                    reuse_weights=True)\n",
    "\n",
    "obs_subset_modres_lazy = xr.map_blocks(apply_weights, regridder_obs_to_mod, \n",
    "                                args=[tmax_obs['tmax']])\n",
    "\n",
    "obs_subset_modres = obs_subset_modres_lazy.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subset datasets to get ready for bias correcting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {'lat': 10, 'lon': 10, 'time': -1}\n",
    "\n",
    "train_subset = ds_model['tasmax'].sel(time=train_slice)\n",
    "train_subset['time'] = train_subset.indexes['time'].to_datetimeindex()\n",
    "train_subset = train_subset.resample(time='1d').mean().load(scheduler='threads').chunk(chunks)\n",
    "\n",
    "\n",
    "holdout_subset = ds_model['tasmax'].sel(time=holdout_slice)\n",
    "holdout_subset['time'] = holdout_subset.indexes['time'].to_datetimeindex()\n",
    "holdout_subset = holdout_subset.resample(time='1d').mean().load(scheduler='threads').chunk(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit BcsdTemperature models at each x/y point in domain using the `PointwiseDownscaler` with the `daily_nasa-nex` option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# model = PointWiseDownscaler(BcsdTemperature(return_anoms=False, time_grouper='daily_nasa-nex'))\n",
    "model = PointWiseDownscaler(BcsdTemperature(return_anoms=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BcsdTemperature(return_anoms=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leap days from model data\n",
    "train_subset_noleap = _remove_leap_days(train_subset)\n",
    "\n",
    "holdout_subset_noleap = _remove_leap_days(holdout_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk datasets \n",
    "train_subset_noleap = train_subset_noleap.chunk(chunks)\n",
    "holdout_subset_noleap = holdout_subset_noleap.chunk(chunks)\n",
    "obs_subset_modres = obs_subset_modres.chunk(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(train_subset_noleap, obs_subset_modres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model, model._models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predicted = model.predict(holdout_subset_noleap).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.isel(time=0).plot(vmin=250, vmax=325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.sel(lat=47, lon=-122, method='nearest').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_predicted = predicted.to_dataset(name='tmax')\n",
    "\n",
    "ds_new_attrs = {\"file description\": \"daily bias correction test for 1980s, output from global bias correction scaling test\",\n",
    "                   \"author\": \"Diana Gergel\", \"contact\": \"dgergel@rhg.com\"}\n",
    "ds_predicted.attrs.update(ds_new_attrs)\n",
    "ds_predicted.to_netcdf(os.path.join(write_direc, 'global_bias_corrected_tenyears.nc'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "downscale_latest_latest",
   "language": "python",
   "name": "downscale_latest_latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
