# Downloads select, raw CMIP6 from GCP to co-located Azure storage for test case.
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: downloadraw-
  labels:
    component: downloadraw
    env: prod
spec:
  entrypoint: main
  arguments:
    parameters:
    - name: gcmruns-url
      value: "https://raw.githubusercontent.com/ClimateImpactLab/downscaleCMIP6/master/workflows/cmip6run-targets.json"
  parallelism: 50  # Limit number of parallel jobs.
  templates:


  - name: main
    inputs:
      parameters:
      - name: gcmruns-url
    steps:
    - - name: fetch-gcmruns-list
        template: fetch-gcmruns-list
        arguments:
          parameters:
          - name: url
            value: "{{ inputs.parameters.gcmruns-url }}"
    - - name: download-gcm
        template: download-gcm
        arguments:
          parameters:
          - name: activity-id
            value: "{{item.activity_id}}"
          - name: experiment-id
            value: "{{item.experiment_id}}"
          - name: table-id
            value: "{{item.table_id}}"
          - name: variable-id
            value: "{{item.variable_id}}"
          - name: source-id
            value: "{{item.source_id}}"
          - name: institution-id
            value: "{{item.institution_id}}"
          - name: member-id
            value: "{{item.member_id}}"
          - name: grid-label
            value: "{{item.grid_label}}"
          - name: version
            value: "{{item.version}}"
          - name: outpath
            value: "raw/{{item.source_id}}/{{item.experiment_id}}/{{item.member_id}}/{{item.variable_id}}/{{item.grid_label}}/{{item.version}}.zarr"
        withParam: "{{ steps.fetch-gcmruns-list.outputs.parameters.gcm-runs }}"


  - name: fetch-gcmruns-list
    inputs:
      parameters:
      - name: url
    script:
      image: appropriate/curl
      command: [sh]
      source: |
        curl -s "{{ inputs.parameters.url }}" > /mnt/out/gcmruns.json
      volumeMounts:
      - name: out
        mountPath: /mnt/out
      resources:
        requests:
          memory: 5Mi
          cpu: "100m"
        limits:
          memory: 10Mi
          cpu: "500m"
    outputs:
      parameters:
      - name: gcm-runs
        valueFrom:
          path: /mnt/out/gcmruns.json
    volumes:
    - name: out
      emptyDir: { }
    activeDeadlineSeconds: 300
    retryStrategy:
      limit: 3


  - name: download-gcm
    inputs:
      parameters:
      - name: activity-id
      - name: experiment-id
      - name: table-id
      - name: variable-id
      - name: source-id
      - name: institution-id
      - name: member-id
      - name: grid-label
      - name: version
      - name: outpath
    script:
      image: pangeo/pangeo-notebook:2021.01.24
      env:
      - name: ACTIVITY_ID
        value: "{{inputs.parameters.activity-id}}"
      - name: EXPERIMENT_ID
        value: "{{inputs.parameters.experiment-id}}"
      - name: TABLE_ID
        value: "{{inputs.parameters.table-id}}"
      - name: VARIABLE_ID
        value: "{{inputs.parameters.variable-id}}"
      - name: SOURCE_ID
        value: "{{inputs.parameters.source-id}}"
      - name: INSTITUTION_ID
        value: "{{inputs.parameters.institution-id}}"
      - name: MEMBER_ID
        value: "{{inputs.parameters.member-id}}"
      - name: GRID_LABEL
        value: "{{inputs.parameters.grid-label}}"
      - name: INTAKE_VERSION
        value: "{{inputs.parameters.version}}"
      - name: OUTPATH
        value: "{{inputs.parameters.outpath}}"
      - name: AZURE_STORAGE_ACCOUNT
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestorageaccount
      - name: AZURE_STORAGE_KEY
        valueFrom:
          secretKeyRef:
            name: workerstoragecreds-secret
            key: azurestoragekey
      command: [/srv/conda/envs/notebook/bin/python]
      source: |
        import os
        import intake
        from adlfs import AzureBlobFileSystem

        print("Searching catalog")
        col = intake.open_esm_datastore("https://storage.googleapis.com/cmip6/pangeo-cmip6.json")
        cat = col.search(
            activity_id=os.environ.get("ACTIVITY_ID"),
            experiment_id=os.environ.get("EXPERIMENT_ID"),
            table_id=os.environ.get("TABLE_ID"),
            variable_id=os.environ.get("VARIABLE_ID"),
            source_id=os.environ.get("SOURCE_ID"),
            member_id=os.environ.get("MEMBER_ID"),
            grid_label=os.environ.get("GRID_LABEL"),
            version=int(os.environ.get("INTAKE_VERSION")),
        )
        d = cat.to_dataset_dict(progressbar=False)
        k = list(d.keys())
        if len(k) != 1:
            raise ValueError("catalog does not have one entry, reconsider input IDs so only one entry")
        print(f"Found one catalog entry {k}")

        print(d)  # DEBUG

        fs = AzureBlobFileSystem(
            account_name=os.environ.get("AZURE_STORAGE_ACCOUNT", None),
            account_key=os.environ.get("AZURE_STORAGE_KEY", None),
            client_id=os.environ.get("AZURE_CLIENT_ID", None),
            client_secret=os.environ.get("AZURE_CLIENT_SECRET", None),
            tenant_id=os.environ.get("AZURE_TENANT_ID", None),
        )
        print("Authenticated with storage")

        store = fs.get_mapper(os.environ.get("OUTPATH"))
        d[k[0]].to_zarr(store, mode="w", compute=True, consolidated=True)
        print(f"Output written to {os.environ.get('OUTPATH')}")
      resources:
        requests:
          memory: 0.7Gi
          cpu: "200m"
        limits:
          memory: 1Gi
          cpu: "1000m"
    activeDeadlineSeconds: 1500
    retryStrategy:
      limit: 4
      retryPolicy: "Always"
